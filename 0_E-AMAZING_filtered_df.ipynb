{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe77a7c-04db-4c01-ad33-a42d61b8d52f",
   "metadata": {},
   "source": [
    "# MSPR : e-amazing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92bcd8-9327-43c6-8c9f-0ce254a9ddc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0. En Amont filtrage de tous les utilisateurs qui se terminent par 1 pour alléger les exécutions par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937c10a-50da-44bf-96df-25b849ddd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = [\n",
    "#     'data/2019-Oct.csv',\n",
    "#     'data/2019-Nov.csv',\n",
    "#     'data/2019-Dec.csv',\n",
    "#     'data/2020-Jan.csv',\n",
    "#     'data/2020-Feb.csv',\n",
    "#     'data/2020-Mar.csv',\n",
    "#     'data/2020-Apr.csv'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea292803-62ad-491b-9f80-f8c673f778d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = [spark.read.csv(file, header=True, inferSchema=True) for file in file_list]\n",
    "\n",
    "# Combiner les DataFrames en un seul\n",
    "combined_df = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Filtrer les données où user_id se termine par '1'\n",
    "filtered_df = combined_df.filter(combined_df['user_id'].cast(\"string\").endswith('1'))\n",
    "\n",
    "\n",
    "\n",
    "# Spécifiez le chemin où vous voulez sauvegarder le fichier Parquet\n",
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "\n",
    "# Enregistrez le DataFrame en Parquet\n",
    "filtered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "\n",
    "# Afficher quelques lignes du DataFrame filtré\n",
    "filtered_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cbcf0-113d-4dca-8f50-9eccc94437d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf169b90-b5c5-4a98-908b-f89ce329daad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd55488d-c58d-4397-93bd-44c55bacf93c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Configuration de la session Spark et chargement des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ebc008b-7891-4047-9477-dd8fa74f634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, count, sum, avg, max as spark_max, datediff, when, countDistinct, row_number, round, dayofweek\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112544ee-b747-40e2-8376-535b3bf1efb4",
   "metadata": {},
   "source": [
    "### Créer une session Spark avec des configurations optimisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee566f90-d0c2-48ce-9db4-7120ce1cac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/28 09:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-commerce Amazing Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68721106-58cb-410d-9166-6b5a477da9d6",
   "metadata": {},
   "source": [
    "### Lecture du fichier Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6808c66a-75e5-46ed-b7d8-590af9793c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "filtered_df = spark.read.parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab5eb2-3459-4314-998e-4cfc562ecf19",
   "metadata": {},
   "source": [
    "### Conversion du champ event_time en type timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fc3382-8f71-4d5d-9d90-7b1089466aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ec3f1-d0c0-4356-949b-988f2c22e061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Création et chargement des mappings pour category_code et brand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9491ba0-de8b-4163-9dc6-008164a510f9",
   "metadata": {},
   "source": [
    "### Extraire les paires uniques product_id, category_id et category_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7c5c1f-68fa-4c11-b14d-5912feb023d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_category_mapping_df = filtered_df.select(\"product_id\", \"category_id\", \"category_code\").distinct()\n",
    "brand_mapping_df = filtered_df.select(\"product_id\", \"brand\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c48093-1b9b-467a-bf8a-d6402b328edf",
   "metadata": {},
   "source": [
    "### Sauvegarder ces mappings dans des fichiers Parquet pour une utilisation ultérieure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b4e1d9-0357-4721-a605-3b961a8d27c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "product_category_mapping_output_path = \"/home/jovyan/work/product_category_mapping.parquet\"\n",
    "brand_mapping_output_path = \"/home/jovyan/work/brand_mapping.parquet\"\n",
    "product_category_mapping_df.write.mode(\"overwrite\").parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df.write.mode(\"overwrite\").parquet(brand_mapping_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819e965-e3b8-4f4c-8552-e642ec2c3b71",
   "metadata": {},
   "source": [
    "### Charger les mappings depuis les fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cceede5-c479-461f-a520-385757d68672",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_category_mapping_df = spark.read.parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df = spark.read.parquet(brand_mapping_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a57ce-fc88-410e-bf64-8bd0b430bfad",
   "metadata": {},
   "source": [
    "### Renommer les colonnes dans les DataFrames de mapping pour éviter l'ambiguïté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b78e7785-04ce-41b3-a461-f6f82fbb643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_category_mapping_df = product_category_mapping_df.withColumnRenamed(\"category_code\", \"mapped_category_code\")\n",
    "brand_mapping_df = brand_mapping_df.withColumnRenamed(\"brand\", \"mapped_brand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef08a9-d497-45c7-9251-d8597f7485ae",
   "metadata": {},
   "source": [
    "### Joindre filtered_df avec product_category_mapping_df et brand_mapping_df pour ajouter les colonnes 'mapped_category_code' et 'mapped_brand'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25591823-c3f6-4675-b709-49bc50417956",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_with_mapping = filtered_df.join(product_category_mapping_df, on=\"product_id\", how=\"left\") \\\n",
    "                                      .join(brand_mapping_df, on=\"product_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f02259-14cb-4b21-9d84-f2fc23e75f34",
   "metadata": {},
   "source": [
    "### Remplacer les valeurs NULL dans 'category_code' et 'brand' par les valeurs correspondantes de la jointure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b314c3c5-bbed-459f-9b0b-0b17a07a866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df_with_mapping.withColumn(\n",
    "    \"category_code\",\n",
    "    coalesce(filtered_df_with_mapping[\"category_code\"], filtered_df_with_mapping[\"mapped_category_code\"])\n",
    ").withColumn(\n",
    "    \"brand\",\n",
    "    coalesce(filtered_df_with_mapping[\"brand\"], filtered_df_with_mapping[\"mapped_brand\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc4c1f-9d81-4b44-ad77-9cf99114ec3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Ajout de colonnes supplémentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe074343-31d1-4898-8930-569aca10c6fc",
   "metadata": {},
   "source": [
    "### Ajout des colonnes supplémentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f5563c-33a6-4806-86fe-b01539711848",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.withColumn(\"event_day_of_week\", dayofweek(col(\"event_time\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ab428-b747-457c-904f-54047dd61206",
   "metadata": {},
   "source": [
    "### Définition des périodes de temps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1053d4c6-e7b0-4bb6-ba23-c51d16d0bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "now = filtered_df.select(spark_max(\"event_time\")).collect()[0][0]\n",
    "last_2_months = now - pd.DateOffset(months=2)\n",
    "last_5_months = now - pd.DateOffset(months=5)\n",
    "last_7_months = now - pd.DateOffset(months=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0e7c8-88b7-448b-ab03-e0e71d775b70",
   "metadata": {},
   "source": [
    "\n",
    "### Filtrage des données pour chaque période"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c9560d-41dd-4c68-a404-9585af85d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2m = filtered_df.filter(col(\"event_time\") >= lit(last_2_months))\n",
    "filtered_df_5m = filtered_df.filter(col(\"event_time\") >= lit(last_5_months))\n",
    "filtered_df_7m = filtered_df.filter(col(\"event_time\") >= lit(last_7_months))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b9b24-bf48-409a-b80d-6dbf5eb6169a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Calcul des statistiques par utilisateur et période\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce56d95-4b62-4f8f-ab19-19bde892ff23",
   "metadata": {},
   "source": [
    "### Fonction pour calculer les statistiques par utilisateur et périodes écoulées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53f6728-a2f4-4de8-97ed-7736e358241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_stats(df, period):\n",
    "    views = df.filter(col(\"event_type\") == \"view\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_views_{period}\"))\n",
    "    carts = df.filter(col(\"event_type\") == \"cart\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_carts_{period}\"))\n",
    "    sessions = df.groupBy(\"user_id\").agg(countDistinct(\"user_session\").alias(f\"number_of_sessions_{period}\"))\n",
    "    purchases = df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(\n",
    "        count(\"*\").alias(f\"count_products_{period}\"),\n",
    "        round(avg(\"price\"), 2).alias(f\"avg_price_{period}\")\n",
    "    )\n",
    "    return views.join(carts, \"user_id\").join(sessions, \"user_id\").join(purchases, \"user_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90932b-b597-461b-aa18-1eccbaa220ef",
   "metadata": {},
   "source": [
    "### Calcul des statistiques pour chaque période"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28470507-adcd-4878-89af-fd48b364f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_2m = compute_user_stats(filtered_df_2m, \"2m\")\n",
    "stats_5m = compute_user_stats(filtered_df_5m, \"5m\")\n",
    "stats_7m = compute_user_stats(filtered_df_7m, \"7m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb03033-6908-4767-859b-30703539e047",
   "metadata": {},
   "source": [
    "### Union des statistiques pour les différentes périodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d715cc-f431-4257-9ed7-b49a0b2a3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_all = stats_2m.join(stats_5m, \"user_id\").join(stats_7m, \"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982f8ac-6436-4933-bc67-d8def8911ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Calcul des autres statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d281b-7606-41bc-af4d-e628943d61a2",
   "metadata": {},
   "source": [
    "### Calcul des autres statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84433126-b558-41dc-8c6d-ce8013f135f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_purchase = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(spark_max(\"event_time\").alias(\"last_purchase\"))\n",
    "days_since_last_purchase = last_purchase.withColumn(\"days_since_last_purchase\", datediff(lit(now), col(\"last_purchase\")))\n",
    "\n",
    "total_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(round(sum(\"price\"), 2).alias(\"total_purchase_value\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31945c8a-77ce-466a-acfd-cb03d9d36aba",
   "metadata": {},
   "source": [
    "### Calcul des abandons de panier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6ab545-120a-4449-8eca-a44d1b67726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_abandonments = filtered_df.groupBy(\"user_id\").agg(\n",
    "    (count(when(col(\"event_type\") == \"cart\", True)) - count(when(col(\"event_type\") == \"purchase\", True))).alias(\"cart_abandonments\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f4711-5494-4d5d-923a-29b96a4b8140",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Calcul de la fidélité à une marque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7cf59-4edb-418d-a11b-9354c103e0f5",
   "metadata": {},
   "source": [
    "### Calcul de la fidélité à une marque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea47c6e-c62b-4f35-bd2c-b9cc1a196d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(count(\"*\").alias(\"total_purchases\"))\n",
    "brand_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"brand\").agg(count(\"*\").alias(\"brand_purchases\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34058c42-26a6-4a69-a61b-b2b72c7977b7",
   "metadata": {},
   "source": [
    "### Déterminer la marque la plus achetée par chaque utilisateur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacce236-097f-4d68-95fb-8986481d63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"brand_purchases\").desc())\n",
    "most_purchased_brand = brand_purchases_by_user.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e3424-4660-43c5-bc7c-81a76d9aee77",
   "metadata": {},
   "source": [
    "\n",
    "### Calcul de la fidélité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cca1e3f1-1e24-4880-b0b2-302fab21691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_loyalty = most_purchased_brand.join(total_purchases_by_user, \"user_id\").withColumn(\"brand_loyalty\", round(col(\"brand_purchases\") / col(\"total_purchases\"), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa889cb-d02e-486d-9747-332180d1352c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Détermination de la catégorie la plus achetée\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2749133-3409-4be2-9075-a42b2b343b07",
   "metadata": {},
   "source": [
    "### Déterminer la catégorie la plus achetée par chaque utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8bde950-5c2a-48bf-bf41-71929b629247",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"category_code\").agg(count(\"*\").alias(\"category_purchases\"))\n",
    "most_purchased_category = category_purchases_by_user.withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"category_purchases\").desc()))).filter(col(\"rank\") == 1).drop(\"rank\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f34f7-835b-4f87-98db-c8c02422c9b0",
   "metadata": {},
   "source": [
    "## 8. Jointure de toutes les statistiques et préparation des résultats finaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6583f-c2b0-43ff-810e-944dd8d0b60c",
   "metadata": {},
   "source": [
    "### Joindre toutes les statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d859591c-a693-44ae-baaa-08ab216cd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_stats_df = stats_all.join(days_since_last_purchase, \"user_id\") \\\n",
    "    .join(total_purchase_value, \"user_id\") \\\n",
    "    .join(cart_abandonments, \"user_id\") \\\n",
    "    .join(brand_loyalty.select(\"user_id\", \"brand_loyalty\", \"brand\"), \"user_id\", \"left\") \\\n",
    "    .join(most_purchased_category.select(\"user_id\", \"category_code\"), \"user_id\", \"left\") \\\n",
    "    .select(\"user_id\", \n",
    "            \"number_of_views_2m\", \"number_of_views_5m\", \"number_of_views_7m\", \n",
    "            \"number_of_carts_2m\", \"number_of_carts_5m\", \"number_of_carts_7m\", \n",
    "            \"number_of_sessions_2m\", \"number_of_sessions_5m\", \"number_of_sessions_7m\", \n",
    "            \"count_products_2m\", \"count_products_5m\", \"count_products_7m\", \n",
    "            \"avg_price_2m\", \"avg_price_5m\", \"avg_price_7m\", \n",
    "            \"total_purchase_value\", \"days_since_last_purchase\", \"cart_abandonments\",\n",
    "            \"brand_loyalty\", \"brand\", \"category_code\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77264e80-74d4-484f-b591-e095b9090784",
   "metadata": {},
   "source": [
    "### Renommer les colonnes pour les préférences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f57b4d7-ac23-4ac9-b75f-5235c1cab740",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats_df = user_stats_df.withColumnRenamed(\"brand\", \"preferred_brand\") \\\n",
    "                             .withColumnRenamed(\"category_code\", \"preferred_category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41895c1e-8cd4-4f26-b528-f66154998f60",
   "metadata": {},
   "source": [
    "### Remplacer les valeurs NULL dans 'preferred_brand' et 'preferred_category' par des valeurs par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68b41a4d-61ac-49ee-8ab5-6f54ccf43e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats_df = user_stats_df.withColumn(\"preferred_brand\", coalesce(col(\"preferred_brand\"), lit(\"No Brand\"))) \\\n",
    "                             .withColumn(\"preferred_category\", coalesce(col(\"preferred_category\"), lit(\"No Category\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81ac49-3c89-460d-b91c-93ec55ffe2d6",
   "metadata": {},
   "source": [
    "### Définir des segments d'utilisateurs basés sur la valeur totale des achats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf8369dd-c1c6-4669-9c73-e29dd8f6d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats_df = user_stats_df.withColumn(\"user_segment\", \n",
    "                                         when(col(\"total_purchase_value\") > 1000, \"High\").\n",
    "                                         when((col(\"total_purchase_value\") <= 1000) & (col(\"total_purchase_value\") > 500), \"Medium\").\n",
    "                                         otherwise(\"Low\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b7eff-f5b6-4334-907a-8571eeb6f6c5",
   "metadata": {},
   "source": [
    "### Affichage du DataFrame final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "989ec7f2-7fbf-4eb2-a65f-cc28e29cb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_stats_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc91cd8-c1ce-4969-81d5-a453b816c15f",
   "metadata": {},
   "source": [
    "## 9. Enregistrement et sauvegarde de user_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded623a-671d-4bcf-b5d4-68397c2b0a3d",
   "metadata": {},
   "source": [
    "### Enregistrer et sauvegarder le DataFrame final dans un fichier Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6dbf3ec-5a66-4128-9f51-2bbcbe2cadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_stats_output_path = \"/home/jovyan/work/user_stats_df_output.parquet\"\n",
    "user_stats_df.write.mode(\"overwrite\").parquet(user_stats_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732202f-cd88-4395-aaf4-549ff98b885d",
   "metadata": {},
   "source": [
    "# Analyse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed003e-15f8-4056-9fba-504670f9091a",
   "metadata": {},
   "source": [
    "## Descriptive des colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed89c5-e8db-47e6-af85-2184f3f1ada9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**user_id :** Identifiant unique de chaque utilisateur. \n",
    "\n",
    "**number_of_views_2m, number_of_views_5m, number_of_views_7m :**\n",
    "\n",
    "**number_of_views_2m** : Nombre de fois qu'un utilisateur a consulté des produits au cours des 2 derniers mois.\n",
    "\n",
    "**number_of_views_5m** : Nombre de fois qu'un utilisateur a consulté des produits au cours des 5 derniers mois.\n",
    "\n",
    "**number_of_views_7m** : Nombre de fois qu'un utilisateur a consulté des produits au cours des 7 derniers mois.\n",
    "\n",
    "**number_of_carts_2m, number_of_carts_5m, number_of_carts_7m :**\n",
    "\n",
    "**number_of_carts_2m** : Nombre de produits ajoutés au panier par un utilisateur au cours des 2 derniers mois.\n",
    "\n",
    "**number_of_carts_5m** : Nombre de produits ajoutés au panier par un utilisateur au cours des 5 derniers mois.\n",
    "\n",
    "**number_of_carts_7m** : Nombre de produits ajoutés au panier par un utilisateur au cours des 7 derniers mois.\n",
    "\n",
    "**number_of_sessions_2m, number_of_sessions_5m, number_of_sessions_7m :**\n",
    "\n",
    "**number_of_sessions_2m**: Nombre de sessions de navigation d'un utilisateur au cours des 2 derniers mois.\n",
    "\n",
    "**number_of_sessions_5m** : Nombre de sessions de navigation d'un utilisateur au cours des 5 derniers mois.\n",
    "\n",
    "**number_of_sessions_7m** : Nombre de sessions de navigation d'un utilisateur au cours des 7 derniers mois.\n",
    "\n",
    "**count_products_2m, count_products_5m, count_products_7m :**\n",
    "\n",
    "**count_products_2m** : Nombre de produits achetés par un utilisateur au cours des 2 derniers mois.\n",
    "\n",
    "**count_products_5m** : Nombre de produits achetés par un utilisateur au cours des 5 derniers mois.\n",
    "\n",
    "**count_products_7m** : Nombre de produits achetés par un utilisateur au cours des 7 derniers mois.\n",
    "\n",
    "**avg_price_2m, avg_price_5m, avg_price_7m :**\n",
    "\n",
    "**avg_price_2m** : Prix moyen des produits achetés par un utilisateur au cours des 2 derniers mois.\n",
    "\n",
    "**avg_price_5m**: Prix moyen des produits achetés par un utilisateur au cours des 5 derniers mois.\n",
    "\n",
    "**avg_price_7m** : Prix moyen des produits achetés par un utilisateur au cours des 7 derniers mois.\n",
    "\n",
    "**total_purchase_value** : Valeur totale des achats effectués par un utilisateur sur l'ensemble de la période considérée (oct 19 à avril 20 inclus).\n",
    "\n",
    "**days_since_last_purchase** : Nombre de jours écoulés depuis le dernier achat de l'utilisateur.\n",
    "\n",
    "**cart_abandonments**: Nombre de produits ajoutés au panier mais non achetés par un utilisateur. Cela mesure les abandons de panier, **indiquant des intentions d'achat non réalisées**.\n",
    "\n",
    "**brand_loyalty** : Fidélité à une marque spécifique. Cette métrique est calculée comme la proportion des achats effectués auprès de la marque la plus fréquemment achetée par rapport au total des achats de l'utilisateur.\n",
    "\n",
    "**preferred_brand** : Marque la plus fréquemment achetée par un utilisateur. S'il n'y a pas de données d'achat pour déterminer une marque préférée, la valeur par défaut est **\"No Brand\"**.\n",
    "\n",
    "**preferred_category** : Catégorie de produit la plus fréquemment achetée par un utilisateur. S'il n'y a pas de données d'achat pour déterminer une catégorie préférée, la valeur par défaut est **\"No Category\"** .\n",
    "\n",
    "**user_segment**: Segment d'utilisateur basé sur la valeur totale des achats. Il peut être catégorisé en :\n",
    "\n",
    "- High : Utilisateurs dont la valeur totale des achats est supérieure à 1000.\n",
    "\n",
    "- Medium : Utilisateurs dont la valeur totale des achats est entre 500 et 1000.\n",
    "  \n",
    "- Low : Utilisateurs dont la valeur totale des achats est inférieure à 500.\n",
    "\n",
    "**avg_session_duration** : Combien de temps les utilisateurs passent sur le site, exrimés en minutes.\n",
    "\n",
    "**most_active_day** : Préférences de l'utilisateur en termes de jours de navigation.\n",
    "\n",
    "**most_active_time** : Utilisateurs en fonction de leurs habitudes de navigation quotidienne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0437ec-d889-4ada-ab41-e7b36fd5f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-16.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682c7f88-6ea3-489e-bbf3-1269ce084c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id  number_of_views_2m  number_of_views_5m  number_of_views_7m  \\\n",
      "0  359242441                 148                 148                 148   \n",
      "1  438263431                 253                 641                 655   \n",
      "2  439490571                  58                  58                  58   \n",
      "3  469107971                   5                   5                   5   \n",
      "4  472393541                   8                  18                  18   \n",
      "\n",
      "   number_of_carts_2m  number_of_carts_5m  number_of_carts_7m  \\\n",
      "0                   8                   8                   8   \n",
      "1                   6                   8                   8   \n",
      "2                   4                   4                   4   \n",
      "3                   2                   2                   2   \n",
      "4                   2                   2                   2   \n",
      "\n",
      "   number_of_sessions_2m  number_of_sessions_5m  number_of_sessions_7m  ...  \\\n",
      "0                      9                      9                      9  ...   \n",
      "1                     46                    123                    128  ...   \n",
      "2                      6                      6                      6  ...   \n",
      "3                      1                      1                      1  ...   \n",
      "4                      2                      4                      4  ...   \n",
      "\n",
      "   avg_price_2m  avg_price_5m  avg_price_7m  total_purchase_value  \\\n",
      "0         56.63         56.63         56.63                113.26   \n",
      "1        120.26        230.12        230.12               1380.73   \n",
      "2         80.83         80.83         80.83                161.66   \n",
      "3        229.50        229.50        229.50                459.00   \n",
      "4        408.66        408.66        408.66                817.32   \n",
      "\n",
      "   days_since_last_purchase  cart_abandonments  brand_loyalty  \\\n",
      "0                        19                  6           1.00   \n",
      "1                        11                  2           0.33   \n",
      "2                        22                  2           1.00   \n",
      "3                         5                  0           1.00   \n",
      "4                        44                  0           1.00   \n",
      "\n",
      "   preferred_brand        preferred_category  user_segment  \n",
      "0          tp-link   furniture.kitchen.table           Low  \n",
      "1          samsung             apparel.shoes          High  \n",
      "2         No Brand            apparel.tshirt           Low  \n",
      "3         No Brand               No Category           Low  \n",
      "4            apple  construction.tools.light        Medium  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Définir le chemin du fichier Parquet\n",
    "user_stats_input_path = \"/home/jovyan/work/user_stats_df_output.parquet\"\n",
    "\n",
    "# Lire le fichier Parquet en utilisant pandas\n",
    "user_stats_df = pd.read_parquet(user_stats_input_path, engine='pyarrow')\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier la lecture\n",
    "print(user_stats_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4759bb2-4715-4069-bdff-9dc49c7b07a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84864, 23)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9382e2ca-dc23-45c9-b62c-cbc4d103ecb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_stats_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc830ce-65c1-458e-b51c-c403a929c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/29 08:47:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/29 08:48:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------+---------------------+---------------------+-----------------+-----------------+-----------------+------------+------------+------------+--------------------+------------------------+-----------------+-------------+---------------+--------------------+--------------------+---------------+----------------+------------+\n",
      "|  user_id|number_of_views_2m|number_of_views_5m|number_of_views_7m|number_of_carts_2m|number_of_carts_5m|number_of_carts_7m|number_of_sessions_2m|number_of_sessions_5m|number_of_sessions_7m|count_products_2m|count_products_5m|count_products_7m|avg_price_2m|avg_price_5m|avg_price_7m|total_purchase_value|days_since_last_purchase|cart_abandonments|brand_loyalty|preferred_brand|  preferred_category|avg_session_duration|most_active_day|most_active_time|user_segment|\n",
      "+---------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------+---------------------+---------------------+-----------------+-----------------+-----------------+------------+------------+------------+--------------------+------------------------+-----------------+-------------+---------------+--------------------+--------------------+---------------+----------------+------------+\n",
      "|359242441|               148|               148|               148|                 8|                 8|                 8|                    9|                    9|                    9|                2|                2|                2|       56.63|       56.63|       56.63|              113.26|                      19|                6|          1.0|        tp-link|furniture.kitchen...|                10.8|              6|         morning|   Low Value|\n",
      "|438263431|               253|               641|               655|                 6|                 8|                 8|                   46|                  123|                  128|                4|                6|                6|      120.26|      230.12|      230.12|             1380.73|                      11|                2|         0.33|        samsung|       apparel.shoes|                3.56|              7|         evening|  High Value|\n",
      "|469107971|                 5|                 5|                 5|                 2|                 2|                 2|                    1|                    1|                    1|                2|                2|                2|       229.5|       229.5|       229.5|               459.0|                       5|                0|          1.0|       No Brand|         No Category|                7.75|              7|       afternoon|   Low Value|\n",
      "|472393541|                 8|                18|                18|                 2|                 2|                 2|                    2|                    4|                    4|                2|                2|                2|      408.66|      408.66|      408.66|              817.32|                      44|                0|          1.0|          apple|construction.tool...|                1.35|              3|         morning|Medium Value|\n",
      "|478219391|                36|                62|                64|                16|                18|                18|                    4|                    6|                    7|                2|                2|                2|      254.58|      254.58|      254.58|              509.16|                       6|               16|          1.0|        samsung|apparel.shoes.sli...|               32.54|              2|         morning|Medium Value|\n",
      "|480165611|                11|               191|               211|                 4|                12|                14|                    5|                   52|                   59|                4|                8|                8|      257.77|       209.9|       209.9|             1679.22|                      24|                6|          0.5|         orient|  electronics.clocks|                0.54|              5|         evening|  High Value|\n",
      "|487774501|                 6|                 8|                10|                 2|                 2|                 2|                    1|                    2|                    3|                2|                2|                2|      241.84|      241.84|      241.84|              483.68|                      58|                0|          1.0|        samsung|construction.tool...|                2.11|              3|         evening|   Low Value|\n",
      "|493917651|                 8|                30|                30|                 2|                 2|                 2|                    1|                    8|                    8|                2|                2|                2|       359.6|       359.6|       359.6|               719.2|                      37|                0|          1.0|        samsung|construction.tool...|               56.87|              3|         morning|Medium Value|\n",
      "|495221371|                60|               358|               358|                 8|                12|                12|                    5|                   30|                   30|                2|                2|                2|      115.81|      115.81|      115.81|              231.62|                      58|               10|          1.0|        karcher|appliances.enviro...|               15.96|              3|         morning|   Low Value|\n",
      "|497578671|                10|                10|                10|                 2|                 2|                 2|                    3|                    3|                    3|                2|                2|                2|        49.4|        49.4|        49.4|                98.8|                      57|                0|          1.0|         nokian|         No Category|                1.78|              4|         morning|   Low Value|\n",
      "|498380681|               175|               343|               357|                 4|                14|                14|                   28|                   73|                   77|                2|                2|                2|       54.06|       54.06|       54.06|              108.12|                      35|               12|          1.0|       cordiant|         No Category|                6.37|              5|       afternoon|   Low Value|\n",
      "|512360471|               741|               743|               743|                16|                16|                16|                   10|                   11|                   11|               10|               10|               10|      201.81|      201.81|      201.81|             2018.06|                      16|                6|          0.6|         huawei|construction.tool...|             1171.64|              3|         morning|  High Value|\n",
      "|512363741|               304|               946|              1322|                18|                61|                61|                   16|                   71|                  100|                4|                9|               10|      185.82|      259.71|      237.93|             2379.34|                      54|               51|          0.4|        fitwell|appliances.enviro...|             2300.88|              1|         morning|  High Value|\n",
      "|512364671|               103|               155|               375|                 4|                 8|                10|                    9|                   12|                   21|                2|                4|                4|      190.42|      121.41|      121.41|              485.64|                       2|                6|          0.5|       No Brand|construction.tool...|                16.1|              3|         morning|   Low Value|\n",
      "|512368481|                80|               139|               171|                19|                19|                21|                   16|                   30|                   35|                6|                6|                8|     1400.27|     1400.27|     1090.18|             8721.44|                      57|               13|         0.75|           asus|electronics.audio...|                2.14|              2|         morning|  High Value|\n",
      "|512369671|               196|               196|               208|                10|                10|                10|                   13|                   13|                   14|                6|                6|                6|        9.61|        9.61|        9.61|               57.68|                       8|                4|         0.67|         canyon|furniture.kitchen...|               12.84|              4|       afternoon|   Low Value|\n",
      "|512371641|                66|                86|               114|                 4|                 4|                 6|                    8|                   15|                   23|                2|                2|                4|     1749.85|     1749.85|       926.4|             3705.58|                       1|                2|          0.5|          apple|appliances.kitche...|             6250.81|              4|         evening|  High Value|\n",
      "|512372691|                50|               126|               174|                18|                32|                41|                    8|                   15|                   21|               16|               30|               41|      130.12|      170.65|      152.52|             6253.37|                       3|                0|         0.34|        samsung|         No Category|                6.43|              1|         morning|  High Value|\n",
      "|512373761|               144|               151|               169|                16|                16|                16|                    4|                    6|                   11|               12|               12|               12|       376.7|       376.7|       376.7|             4520.36|                       4|                4|         0.83|        samsung|furniture.bedroom...|                7.07|              7|         morning|  High Value|\n",
      "|512375521|                 8|                 8|                 8|                12|                12|                12|                    2|                    2|                    2|                2|                2|                2|      437.57|      437.57|      437.57|              875.14|                      59|               10|          1.0|         lenovo|electronics.audio...|                2.43|              2|         morning|Medium Value|\n",
      "+---------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------+---------------------+---------------------+-----------------+-----------------+-----------------+------------+------------+------------+--------------------+------------------------+-----------------+-------------+---------------+--------------------+--------------------+---------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, count, sum, avg, max as spark_max, min as spark_min, datediff, when, countDistinct, row_number, round, dayofweek, hour\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Configuration de la session Spark et chargement des données\n",
    "# Créer une session Spark avec des configurations optimisées\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-commerce Amazing Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le chemin du fichier Parquet\n",
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "\n",
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Conversion du champ event_time en type timestamp\n",
    "filtered_df = filtered_df.withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\"))\n",
    "\n",
    "# 2. Création et chargement des mappings pour `category_code` et `brand`\n",
    "# Extraire les paires uniques product_id, category_id et category_code\n",
    "product_category_mapping_df = filtered_df.select(\"product_id\", \"category_id\", \"category_code\").distinct()\n",
    "brand_mapping_df = filtered_df.select(\"product_id\", \"brand\").distinct()\n",
    "\n",
    "# Sauvegarder ces mappings dans des fichiers Parquet pour une utilisation ultérieure\n",
    "product_category_mapping_output_path = \"/home/jovyan/work/product_category_mapping.parquet\"\n",
    "brand_mapping_output_path = \"/home/jovyan/work/brand_mapping.parquet\"\n",
    "product_category_mapping_df.write.mode(\"overwrite\").parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df.write.mode(\"overwrite\").parquet(brand_mapping_output_path)\n",
    "\n",
    "# Charger les mappings depuis les fichiers Parquet\n",
    "product_category_mapping_df = spark.read.parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df = spark.read.parquet(brand_mapping_output_path)\n",
    "\n",
    "# Renommer les colonnes dans les DataFrames de mapping pour éviter l'ambiguïté\n",
    "product_category_mapping_df = product_category_mapping_df.withColumnRenamed(\"category_code\", \"mapped_category_code\")\n",
    "brand_mapping_df = brand_mapping_df.withColumnRenamed(\"brand\", \"mapped_brand\")\n",
    "\n",
    "# Joindre filtered_df avec product_category_mapping_df et brand_mapping_df pour ajouter les colonnes 'mapped_category_code' et 'mapped_brand'\n",
    "filtered_df_with_mapping = filtered_df.join(product_category_mapping_df, on=\"product_id\", how=\"left\") \\\n",
    "                                      .join(brand_mapping_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'category_code' et 'brand' par les valeurs correspondantes de la jointure\n",
    "filtered_df = filtered_df_with_mapping.withColumn(\n",
    "    \"category_code\",\n",
    "    coalesce(filtered_df_with_mapping[\"category_code\"], filtered_df_with_mapping[\"mapped_category_code\"])\n",
    ").withColumn(\n",
    "    \"brand\",\n",
    "    coalesce(filtered_df_with_mapping[\"brand\"], filtered_df_with_mapping[\"mapped_brand\"])\n",
    ")\n",
    "\n",
    "# 3. Ajout de colonnes supplémentaires\n",
    "# Ajout des colonnes supplémentaires\n",
    "filtered_df = filtered_df.withColumn(\"event_day_of_week\", dayofweek(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_hour\", hour(col(\"event_time\")))\n",
    "\n",
    "# Définition des périodes de temps\n",
    "now = filtered_df.select(spark_max(\"event_time\")).collect()[0][0]\n",
    "last_2_months = now - pd.DateOffset(months=2)\n",
    "last_5_months = now - pd.DateOffset(months=5)\n",
    "last_7_months = now - pd.DateOffset(months=7)\n",
    "\n",
    "# Filtrage des données pour chaque période\n",
    "filtered_df_2m = filtered_df.filter(col(\"event_time\") >= lit(last_2_months))\n",
    "filtered_df_5m = filtered_df.filter(col(\"event_time\") >= lit(last_5_months))\n",
    "filtered_df_7m = filtered_df.filter(col(\"event_time\") >= lit(last_7_months))\n",
    "\n",
    "# 4. Calcul des statistiques par utilisateur et période\n",
    "# Fonction pour calculer les statistiques par utilisateur et période\n",
    "def compute_user_stats(df, period):\n",
    "    views = df.filter(col(\"event_type\") == \"view\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_views_{period}\"))\n",
    "    carts = df.filter(col(\"event_type\") == \"cart\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_carts_{period}\"))\n",
    "    sessions = df.groupBy(\"user_id\").agg(countDistinct(\"user_session\").alias(f\"number_of_sessions_{period}\"))\n",
    "    purchases = df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(\n",
    "        count(\"*\").alias(f\"count_products_{period}\"),\n",
    "        round(avg(\"price\"), 2).alias(f\"avg_price_{period}\")\n",
    "    )\n",
    "    return views.join(carts, \"user_id\").join(sessions, \"user_id\").join(purchases, \"user_id\")\n",
    "\n",
    "# Calcul des statistiques pour chaque période\n",
    "stats_2m = compute_user_stats(filtered_df_2m, \"2m\")\n",
    "stats_5m = compute_user_stats(filtered_df_5m, \"5m\")\n",
    "stats_7m = compute_user_stats(filtered_df_7m, \"7m\")\n",
    "\n",
    "# Union des statistiques pour les différentes périodes\n",
    "stats_all = stats_2m.join(stats_5m, \"user_id\").join(stats_7m, \"user_id\")\n",
    "\n",
    "# 5. Calcul des autres statistiques\n",
    "# Calcul des autres statistiques\n",
    "last_purchase = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(spark_max(\"event_time\").alias(\"last_purchase\"))\n",
    "days_since_last_purchase = last_purchase.withColumn(\"days_since_last_purchase\", datediff(lit(now), col(\"last_purchase\")))\n",
    "\n",
    "total_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(round(sum(\"price\"), 2).alias(\"total_purchase_value\"))\n",
    "\n",
    "# Calcul des abandons de panier\n",
    "cart_abandonments = filtered_df.groupBy(\"user_id\").agg(\n",
    "    (count(when(col(\"event_type\") == \"cart\", True)) - count(when(col(\"event_type\") == \"purchase\", True))).alias(\"cart_abandonments\")\n",
    ")\n",
    "\n",
    "# 6. Calcul de la fidélité à une marque\n",
    "# Calcul de la fidélité à une marque\n",
    "total_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(count(\"*\").alias(\"total_purchases\"))\n",
    "brand_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"brand\").agg(count(\"*\").alias(\"brand_purchases\"))\n",
    "\n",
    "# Déterminer la marque la plus achetée par chaque utilisateur\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"brand_purchases\").desc())\n",
    "most_purchased_brand = brand_purchases_by_user.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Calcul de la fidélité\n",
    "brand_loyalty = most_purchased_brand.join(total_purchases_by_user, \"user_id\").withColumn(\"brand_loyalty\", round(col(\"brand_purchases\") / col(\"total_purchases\"), 2))\n",
    "\n",
    "# 7. Détermination de la catégorie la plus achetée\n",
    "# Déterminer la catégorie la plus achetée par chaque utilisateur\n",
    "category_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"category_code\").agg(count(\"*\").alias(\"category_purchases\"))\n",
    "most_purchased_category = category_purchases_by_user.withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"category_purchases\").desc()))).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# 8. Calcul du temps passé sur une session et du moment le plus actif\n",
    "session_times = filtered_df.groupBy(\"user_id\", \"user_session\").agg(\n",
    "    (spark_max(\"event_time\").cast(\"long\") - spark_min(\"event_time\").cast(\"long\")).alias(\"session_duration\")\n",
    ").groupBy(\"user_id\").agg(\n",
    "    round(avg(\"session_duration\") / 60, 2).alias(\"avg_session_duration\")  # moyenne de la durée des sessions en minutes\n",
    ")\n",
    "\n",
    "most_active_day = filtered_df.groupBy(\"user_id\", \"event_day_of_week\").agg(count(\"*\").alias(\"day_activity\")) \\\n",
    "                             .withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"day_activity\").desc()))) \\\n",
    "                             .filter(col(\"rank\") == 1).drop(\"rank\") \\\n",
    "                             .select(\"user_id\", col(\"event_day_of_week\").alias(\"most_active_day\"))\n",
    "\n",
    "most_active_time = filtered_df.withColumn(\"time_of_day\",\n",
    "    when(col(\"event_hour\").between(5, 12), \"morning\")\n",
    "    .when(col(\"event_hour\").between(12, 17), \"afternoon\")\n",
    "    .otherwise(\"evening\")\n",
    ").groupBy(\"user_id\", \"time_of_day\").agg(count(\"*\").alias(\"time_activity\")) \\\n",
    "  .withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"time_activity\").desc()))) \\\n",
    "  .filter(col(\"rank\") == 1).drop(\"rank\") \\\n",
    "  .select(\"user_id\", col(\"time_of_day\").alias(\"most_active_time\"))\n",
    "\n",
    "# 9. Jointure de toutes les statistiques et préparation des résultats finaux\n",
    "# Joindre toutes les statistiques\n",
    "user_stats_df = stats_all.join(days_since_last_purchase, \"user_id\") \\\n",
    "    .join(total_purchase_value, \"user_id\") \\\n",
    "    .join(cart_abandonments, \"user_id\") \\\n",
    "    .join(brand_loyalty.select(\"user_id\", \"brand_loyalty\", \"brand\"), \"user_id\", \"left\") \\\n",
    "    .join(most_purchased_category.select(\"user_id\", \"category_code\"), \"user_id\", \"left\") \\\n",
    "    .join(session_times, \"user_id\", \"left\") \\\n",
    "    .join(most_active_day, \"user_id\", \"left\") \\\n",
    "    .join(most_active_time, \"user_id\", \"left\") \\\n",
    "    .select(\"user_id\", \n",
    "            \"number_of_views_2m\", \"number_of_views_5m\", \"number_of_views_7m\", \n",
    "            \"number_of_carts_2m\", \"number_of_carts_5m\", \"number_of_carts_7m\", \n",
    "            \"number_of_sessions_2m\", \"number_of_sessions_5m\", \"number_of_sessions_7m\", \n",
    "            \"count_products_2m\", \"count_products_5m\", \"count_products_7m\", \n",
    "            \"avg_price_2m\", \"avg_price_5m\", \"avg_price_7m\", \n",
    "            \"total_purchase_value\", \"days_since_last_purchase\", \"cart_abandonments\",\n",
    "            \"brand_loyalty\", \"brand\", \"category_code\", \"avg_session_duration\",\n",
    "            \"most_active_day\", \"most_active_time\")\n",
    "\n",
    "# Renommer les colonnes pour les préférences\n",
    "user_stats_df = user_stats_df.withColumnRenamed(\"brand\", \"preferred_brand\") \\\n",
    "                             .withColumnRenamed(\"category_code\", \"preferred_category\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'preferred_brand' et 'preferred_category' par des valeurs par défaut\n",
    "user_stats_df = user_stats_df.withColumn(\"preferred_brand\", coalesce(col(\"preferred_brand\"), lit(\"No Brand\"))) \\\n",
    "                             .withColumn(\"preferred_category\", coalesce(col(\"preferred_category\"), lit(\"No Category\")))\n",
    "\n",
    "# Définir des segments d'utilisateurs basés sur la valeur totale des achats\n",
    "user_stats_df = user_stats_df.withColumn(\"user_segment\", \n",
    "                                         when(col(\"total_purchase_value\") > 1000, \"High\").\n",
    "                                         when((col(\"total_purchase_value\") <= 1000) & (col(\"total_purchase_value\") > 500), \"Medium\").\n",
    "                                         otherwise(\"Low\"))\n",
    "\n",
    "# Affichage du DataFrame final\n",
    "user_stats_df.show()\n",
    "\n",
    "# 10. Enregistrement et sauvegarde de `user_stats_df`\n",
    "# Définir le chemin de sortie pour le DataFrame final\n",
    "user_stats_output_path = \"/home/jovyan/work/user_stats_df_output.parquet\"\n",
    "\n",
    "# Enregistrer et sauvegarder le DataFrame final dans un fichier Parquet\n",
    "user_stats_df.write.mode(\"overwrite\").parquet(user_stats_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0579ede-c5be-4fb6-a1a1-6d27f77f1fde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[1;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    662\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221794e2-ba97-417b-a021-937c771a723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                       (10 + 2) / 21]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=59>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o685.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "[Stage 1:================================>                        (12 + 2) / 21]\r"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o718.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m product_category_mapping_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/work/product_category_mapping.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m brand_mapping_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/work/brand_mapping.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mproduct_category_mapping_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_category_mapping_output_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m brand_mapping_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(brand_mapping_output_path)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Charger les mappings depuis les fichiers Parquet\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o718.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, count, sum, avg, max as spark_max, min as spark_min, datediff, when, countDistinct, row_number, round, dayofweek, hour\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Configuration de la session Spark et chargement des données\n",
    "# Créer une session Spark avec des configurations optimisées\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-commerce Amazing Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le chemin du fichier Parquet\n",
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "\n",
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Conversion du champ event_time en type timestamp\n",
    "filtered_df = filtered_df.withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\"))\n",
    "\n",
    "# 2. Création et chargement des mappings pour `category_code` et `brand`\n",
    "# Extraire les paires uniques product_id, category_id et category_code\n",
    "product_category_mapping_df = filtered_df.select(\"product_id\", \"category_id\", \"category_code\").distinct()\n",
    "brand_mapping_df = filtered_df.select(\"product_id\", \"brand\").distinct()\n",
    "\n",
    "# Sauvegarder ces mappings dans des fichiers Parquet pour une utilisation ultérieure\n",
    "product_category_mapping_output_path = \"/home/jovyan/work/product_category_mapping.parquet\"\n",
    "brand_mapping_output_path = \"/home/jovyan/work/brand_mapping.parquet\"\n",
    "product_category_mapping_df.write.mode(\"overwrite\").parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df.write.mode(\"overwrite\").parquet(brand_mapping_output_path)\n",
    "\n",
    "# Charger les mappings depuis les fichiers Parquet\n",
    "product_category_mapping_df = spark.read.parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df = spark.read.parquet(brand_mapping_output_path)\n",
    "\n",
    "# Renommer les colonnes dans les DataFrames de mapping pour éviter l'ambiguïté\n",
    "product_category_mapping_df = product_category_mapping_df.withColumnRenamed(\"category_code\", \"mapped_category_code\")\n",
    "brand_mapping_df = brand_mapping_df.withColumnRenamed(\"brand\", \"mapped_brand\")\n",
    "\n",
    "# Joindre filtered_df avec product_category_mapping_df et brand_mapping_df pour ajouter les colonnes 'mapped_category_code' et 'mapped_brand'\n",
    "filtered_df_with_mapping = filtered_df.join(product_category_mapping_df, on=\"product_id\", how=\"left\") \\\n",
    "                                      .join(brand_mapping_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'category_code' et 'brand' par les valeurs correspondantes de la jointure\n",
    "filtered_df = filtered_df_with_mapping.withColumn(\n",
    "    \"category_code\",\n",
    "    coalesce(filtered_df_with_mapping[\"category_code\"], filtered_df_with_mapping[\"mapped_category_code\"])\n",
    ").withColumn(\n",
    "    \"brand\",\n",
    "    coalesce(filtered_df_with_mapping[\"brand\"], filtered_df_with_mapping[\"mapped_brand\"])\n",
    ")\n",
    "\n",
    "# 3. Ajout de colonnes supplémentaires\n",
    "# Ajout des colonnes supplémentaires\n",
    "filtered_df = filtered_df.withColumn(\"event_day_of_week\", dayofweek(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_hour\", hour(col(\"event_time\")))\n",
    "\n",
    "# Définition des périodes de temps\n",
    "now = filtered_df.select(spark_max(\"event_time\")).collect()[0][0]\n",
    "last_2_months = now - pd.DateOffset(months=2)\n",
    "last_5_months = now - pd.DateOffset(months=5)\n",
    "last_7_months = now - pd.DateOffset(months=7)\n",
    "\n",
    "# Filtrage des données pour chaque période\n",
    "filtered_df_2m = filtered_df.filter(col(\"event_time\") >= lit(last_2_months))\n",
    "filtered_df_5m = filtered_df.filter(col(\"event_time\") >= lit(last_5_months))\n",
    "filtered_df_7m = filtered_df.filter(col(\"event_time\") >= lit(last_7_months))\n",
    "\n",
    "# 4. Calcul des statistiques par utilisateur et période\n",
    "# Fonction pour calculer les statistiques par utilisateur et période\n",
    "def compute_user_stats(df, period):\n",
    "    views = df.filter(col(\"event_type\") == \"view\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_views_{period}\"))\n",
    "    carts = df.filter(col(\"event_type\") == \"cart\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_carts_{period}\"))\n",
    "    sessions = df.groupBy(\"user_id\").agg(countDistinct(\"user_session\").alias(f\"number_of_sessions_{period}\"))\n",
    "    purchases = df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(\n",
    "        count(\"*\").alias(f\"count_products_{period}\"),\n",
    "        round(avg(\"price\"), 2).alias(f\"avg_price_{period}\")\n",
    "    )\n",
    "    return views.join(carts, \"user_id\").join(sessions, \"user_id\").join(purchases, \"user_id\")\n",
    "\n",
    "# Calcul des statistiques pour chaque période\n",
    "stats_2m = compute_user_stats(filtered_df_2m, \"2m\")\n",
    "stats_5m = compute_user_stats(filtered_df_5m, \"5m\")\n",
    "stats_7m = compute_user_stats(filtered_df_7m, \"7m\")\n",
    "\n",
    "# Union des statistiques pour les différentes périodes\n",
    "stats_all = stats_2m.join(stats_5m, \"user_id\").join(stats_7m, \"user_id\")\n",
    "\n",
    "# 5. Calcul des autres statistiques\n",
    "# Calcul des autres statistiques\n",
    "last_purchase = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(spark_max(\"event_time\").alias(\"last_purchase\"))\n",
    "days_since_last_purchase = last_purchase.withColumn(\"days_since_last_purchase\", datediff(lit(now), col(\"last_purchase\")))\n",
    "\n",
    "total_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(round(sum(\"price\"), 2).alias(\"total_purchase_value\"))\n",
    "\n",
    "# Calcul des abandons de panier\n",
    "cart_events = filtered_df.filter(col(\"event_type\") == \"cart\").groupBy(\"user_id\").agg(count(\"*\").alias(\"cart_count\"))\n",
    "purchase_events = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(count(\"*\").alias(\"purchase_count\"))\n",
    "\n",
    "cart_abandonments = cart_events.join(purchase_events, \"user_id\", \"left\") \\\n",
    "                               .withColumn(\"cart_abandonments\", when(col(\"cart_count\") > col(\"purchase_count\"), col(\"cart_count\") - col(\"purchase_count\")).otherwise(0)) \\\n",
    "                               .select(\"user_id\", \"cart_abandonments\")\n",
    "\n",
    "# 6. Calcul de la fidélité à une marque\n",
    "# Calcul de la fidélité à une marque\n",
    "total_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(count(\"*\").alias(\"total_purchases\"))\n",
    "brand_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"brand\").agg(count(\"*\").alias(\"brand_purchases\"))\n",
    "\n",
    "# Déterminer la marque la plus achetée par chaque utilisateur\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"brand_purchases\").desc())\n",
    "most_purchased_brand = brand_purchases_by_user.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Calcul de la fidélité\n",
    "brand_loyalty = most_purchased_brand.join(total_purchases_by_user, \"user_id\").withColumn(\"brand_loyalty\", round(col(\"brand_purchases\") / col(\"total_purchases\"), 2))\n",
    "\n",
    "# 7. Détermination de la catégorie la plus achetée\n",
    "# Déterminer la catégorie la plus achetée par chaque utilisateur\n",
    "category_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"category_code\").agg(count(\"*\").alias(\"category_purchases\"))\n",
    "most_purchased_category = category_purchases_by_user.withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"category_purchases\").desc()))).filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# 8. Calcul du temps passé sur une session et du moment le plus actif\n",
    "session_times = filtered_df.groupBy(\"user_id\", \"user_session\").agg(\n",
    "    (spark_max(\"event_time\").cast(\"long\") - spark_min(\"event_time\").cast(\"long\")).alias(\"session_duration\")\n",
    ").groupBy(\"user_id\").agg(\n",
    "    round(avg(\"session_duration\") / 60, 2).alias(\"avg_session_duration\")  # moyenne de la durée des sessions en minutes\n",
    ")\n",
    "\n",
    "most_active_day = filtered_df.groupBy(\"user_id\", \"event_day_of_week\").agg(count(\"*\").alias(\"day_activity\")) \\\n",
    "                             .withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"day_activity\").desc()))) \\\n",
    "                             .filter(col(\"rank\") == 1).drop(\"rank\") \\\n",
    "                             .select(\"user_id\", col(\"event_day_of_week\").alias(\"most_active_day\"))\n",
    "\n",
    "most_active_time = filtered_df.withColumn(\"time_of_day\",\n",
    "    when(col(\"event_hour\").between(5, 12), \"morning\")\n",
    "    .when(col(\"event_hour\").between(12, 17), \"afternoon\")\n",
    "    .otherwise(\"evening\")\n",
    ").groupBy(\"user_id\", \"time_of_day\").agg(count(\"*\").alias(\"time_activity\")) \\\n",
    "  .withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"time_activity\").desc()))) \\\n",
    "  .filter(col(\"rank\") == 1).drop(\"rank\") \\\n",
    "  .select(\"user_id\", col(\"time_of_day\").alias(\"most_active_time\"))\n",
    "\n",
    "# 9. Jointure de toutes les statistiques et préparation des résultats finaux\n",
    "# Joindre toutes les statistiques\n",
    "user_stats_df = stats_all.join(days_since_last_purchase, \"user_id\") \\\n",
    "    .join(total_purchase_value, \"user_id\") \\\n",
    "    .join(cart_abandonments, \"user_id\") \\\n",
    "    .join(brand_loyalty.select(\"user_id\", \"brand_loyalty\", \"brand\"), \"user_id\", \"left\") \\\n",
    "    .join(most_purchased_category.select(\"user_id\", \"category_code\"), \"user_id\", \"left\") \\\n",
    "    .join(session_times, \"user_id\", \"left\") \\\n",
    "    .join(most_active_day, \"user_id\", \"left\") \\\n",
    "    .join(most_active_time, \"user_id\", \"left\") \\\n",
    "    .select(\"user_id\", \n",
    "            \"number_of_views_2m\", \"number_of_views_5m\", \"number_of_views_7m\", \n",
    "            \"number_of_carts_2m\", \"number_of_carts_5m\", \"number_of_carts_7m\", \n",
    "            \"number_of_sessions_2m\", \"number_of_sessions_5m\", \"number_of_sessions_7m\", \n",
    "            \"count_products_2m\", \"count_products_5m\", \"count_products_7m\", \n",
    "            \"avg_price_2m\", \"avg_price_5m\", \"avg_price_7m\", \n",
    "            \"total_purchase_value\", \"days_since_last_purchase\", \"cart_abandonments\",\n",
    "            \"brand_loyalty\", \"brand\", \"category_code\", \"avg_session_duration\",\n",
    "            \"most_active_day\", \"most_active_time\")\n",
    "\n",
    "# Renommer les colonnes pour les préférences\n",
    "user_stats_df = user_stats_df.withColumnRenamed(\"brand\", \"preferred_brand\") \\\n",
    "                             .withColumnRenamed(\"category_code\", \"preferred_category\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'preferred_brand' et 'preferred_category' par des valeurs par défaut\n",
    "user_stats_df = user_stats_df.withColumn(\"preferred_brand\", coalesce(col(\"preferred_brand\"), lit(\"No Brand\"))) \\\n",
    "                             .withColumn(\"preferred_category\", coalesce(col(\"preferred_category\"), lit(\"No Category\")))\n",
    "\n",
    "# Définir des segments d'utilisateurs basés sur la valeur totale des achats\n",
    "user_stats_df = user_stats_df.withColumn(\"user_segment\", \n",
    "                                         when(col(\"total_purchase_value\") > 1000, \"High\").\n",
    "                                         when((col(\"total_purchase_value\") <= 1000) & (col(\"total_purchase_value\") > 500), \"Medium\").\n",
    "                                         otherwise(\"Low\"))\n",
    "\n",
    "# Affichage du DataFrame final\n",
    "user_stats_df.show()\n",
    "\n",
    "# 10. Enregistrement et sauvegarde de `user_stats_df`\n",
    "# Définir le chemin de sortie pour le DataFrame final\n",
    "user_stats_output_path = \"/home/jovyan/work/user_stats_df_output.parquet\"\n",
    "\n",
    "# Enregistrer et sauvegarder le DataFrame final dans un fichier Parquet\n",
    "user_stats_df.write.mode(\"overwrite\").parquet(user_stats_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15884085-86e6-4700-9597-4eaaa2162c92",
   "metadata": {},
   "source": [
    "# Vérification valeur négative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea971f40-7318-43d4-97ea-09344377ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs nulles par lots\n",
    "def check_columns_for_nulls(df, columns):\n",
    "    for column in columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        print(f\"Column '{column}' has {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53b8017-2a09-4e90-9f62-a39eb7ef5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_for_negatives(df, columns):\n",
    "    for column in columns:\n",
    "        negative_count = df.filter(col(column) < 0).count()\n",
    "        print(f\"Column '{column}' has {negative_count} negative values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eff4309-ec18-4fe0-b695-22bdaf0f6a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. Configuration de la session Spark et chargement des données\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Créer une session Spark avec des configurations optimisées pour la mémoire\u001b[39;00m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE-commerce Amazing Analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.maxResultSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.instances\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.memory.fraction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.memory.storageFraction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.memory.offHeap.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.memory.offHeap.size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Définir le chemin du fichier Parquet\u001b[39;00m\n\u001b[1;32m     23\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/work/filtered_df_output.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, count, sum, avg, max as spark_max, min as spark_min, datediff, when, countDistinct, row_number, round, dayofweek, hour\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Configuration de la session Spark et chargement des données\n",
    "# Créer une session Spark avec des configurations optimisées pour la mémoire\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-commerce Amazing Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le chemin du fichier Parquet\n",
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "\n",
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path).cache()\n",
    "\n",
    "# Conversion du champ event_time en type timestamp\n",
    "filtered_df = filtered_df.withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\"))\n",
    "\n",
    "# 2. Création et chargement des mappings pour `category_code` et `brand`\n",
    "# Extraire les paires uniques product_id, category_id et category_code\n",
    "product_category_mapping_df = filtered_df.select(\"product_id\", \"category_id\", \"category_code\").distinct().cache()\n",
    "brand_mapping_df = filtered_df.select(\"product_id\", \"brand\").distinct().cache()\n",
    "\n",
    "# Sauvegarder ces mappings dans des fichiers Parquet pour une utilisation ultérieure\n",
    "product_category_mapping_output_path = \"/home/jovyan/work/product_category_mapping.parquet\"\n",
    "brand_mapping_output_path = \"/home/jovyan/work/brand_mapping.parquet\"\n",
    "product_category_mapping_df.write.mode(\"overwrite\").parquet(product_category_mapping_output_path)\n",
    "brand_mapping_df.write.mode(\"overwrite\").parquet(brand_mapping_output_path)\n",
    "\n",
    "# Charger les mappings depuis les fichiers Parquet\n",
    "product_category_mapping_df = spark.read.parquet(product_category_mapping_output_path).cache()\n",
    "brand_mapping_df = spark.read.parquet(brand_mapping_output_path).cache()\n",
    "\n",
    "# Renommer les colonnes dans les DataFrames de mapping pour éviter l'ambiguïté\n",
    "product_category_mapping_df = product_category_mapping_df.withColumnRenamed(\"category_code\", \"mapped_category_code\")\n",
    "brand_mapping_df = brand_mapping_df.withColumnRenamed(\"brand\", \"mapped_brand\")\n",
    "\n",
    "# Joindre filtered_df avec product_category_mapping_df et brand_mapping_df pour ajouter les colonnes 'mapped_category_code' et 'mapped_brand'\n",
    "filtered_df_with_mapping = filtered_df.join(product_category_mapping_df, on=\"product_id\", how=\"left\") \\\n",
    "                                      .join(brand_mapping_df, on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'category_code' et 'brand' par les valeurs correspondantes de la jointure\n",
    "filtered_df = filtered_df_with_mapping.withColumn(\n",
    "    \"category_code\",\n",
    "    coalesce(filtered_df_with_mapping[\"category_code\"], filtered_df_with_mapping[\"mapped_category_code\"])\n",
    ").withColumn(\n",
    "    \"brand\",\n",
    "    coalesce(filtered_df_with_mapping[\"brand\"], filtered_df_with_mapping[\"mapped_brand\"])\n",
    ").cache()\n",
    "\n",
    "# 3. Ajout de colonnes supplémentaires\n",
    "# Ajout des colonnes supplémentaires\n",
    "filtered_df = filtered_df.withColumn(\"event_day_of_week\", dayofweek(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_hour\", hour(col(\"event_time\")))\n",
    "\n",
    "# Définition des périodes de temps\n",
    "now = filtered_df.select(spark_max(\"event_time\")).collect()[0][0]\n",
    "last_2_months = now - pd.DateOffset(months=2)\n",
    "last_5_months = now - pd.DateOffset(months=5)\n",
    "last_7_months = now - pd.DateOffset(months=7)\n",
    "\n",
    "# Filtrage des données pour chaque période\n",
    "filtered_df_2m = filtered_df.filter(col(\"event_time\") >= lit(last_2_months)).cache()\n",
    "filtered_df_5m = filtered_df.filter(col(\"event_time\") >= lit(last_5_months)).cache()\n",
    "filtered_df_7m = filtered_df.filter(col(\"event_time\") >= lit(last_7_months)).cache()\n",
    "\n",
    "# 4. Calcul des statistiques par utilisateur et période\n",
    "# Fonction pour calculer les statistiques par utilisateur et période\n",
    "def compute_user_stats(df, period):\n",
    "    views = df.filter(col(\"event_type\") == \"view\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_views_{period}\"))\n",
    "    carts = df.filter(col(\"event_type\") == \"cart\").groupBy(\"user_id\").agg(count(\"*\").alias(f\"number_of_carts_{period}\"))\n",
    "    sessions = df.groupBy(\"user_id\").agg(countDistinct(\"user_session\").alias(f\"number_of_sessions_{period}\"))\n",
    "    purchases = df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(\n",
    "        count(\"*\").alias(f\"count_products_{period}\"),\n",
    "        round(avg(\"price\"), 2).alias(f\"avg_price_{period}\")\n",
    "    )\n",
    "    return views.join(carts, \"user_id\").join(sessions, \"user_id\").join(purchases, \"user_id\")\n",
    "\n",
    "# Calcul des statistiques pour chaque période\n",
    "stats_2m = compute_user_stats(filtered_df_2m, \"2m\").cache()\n",
    "stats_5m = compute_user_stats(filtered_df_5m, \"5m\").cache()\n",
    "stats_7m = compute_user_stats(filtered_df_7m, \"7m\").cache()\n",
    "\n",
    "# Union des statistiques pour les différentes périodes\n",
    "stats_all = stats_2m.join(stats_5m, \"user_id\").join(stats_7m, \"user_id\").cache()\n",
    "\n",
    "# 5. Calcul des autres statistiques\n",
    "# Calcul des autres statistiques\n",
    "last_purchase = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(spark_max(\"event_time\").alias(\"last_purchase\")).cache()\n",
    "days_since_last_purchase = last_purchase.withColumn(\"days_since_last_purchase\", datediff(lit(now), col(\"last_purchase\"))).cache()\n",
    "\n",
    "total_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(round(sum(\"price\"), 2).alias(\"total_purchase_value\")).cache()\n",
    "\n",
    "# Calcul des abandons de panier\n",
    "cart_events = filtered_df.filter(col(\"event_type\") == \"cart\").groupBy(\"user_id\").agg(count(\"*\").alias(\"cart_count\")).cache()\n",
    "purchase_events = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(count(\"*\").alias(\"purchase_count\")).cache()\n",
    "\n",
    "cart_abandonments = cart_events.join(purchase_events, \"user_id\", \"left\") \\\n",
    "                               .withColumn(\"cart_abandonments\", when(col(\"cart_count\") > col(\"purchase_count\"), col(\"cart_count\") - col(\"purchase_count\")).otherwise(0)) \\\n",
    "                               .select(\"user_id\", \"cart_abandonments\").cache()\n",
    "\n",
    "# 6. Calcul de la fidélité à une marque\n",
    "# Calcul de la fidélité à une marque\n",
    "total_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\").agg(count(\"*\").alias(\"total_purchases\")).cache()\n",
    "brand_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"brand\").agg(count(\"*\").alias(\"brand_purchases\")).cache()\n",
    "\n",
    "# Déterminer la marque la plus achetée par chaque utilisateur\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"brand_purchases\").desc())\n",
    "most_purchased_brand = brand_purchases_by_user.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1).drop(\"rank\").cache()\n",
    "\n",
    "# Calcul de la fidélité\n",
    "brand_loyalty = most_purchased_brand.join(total_purchases_by_user, \"user_id\").withColumn(\"brand_loyalty\", round(col(\"brand_purchases\") / col(\"total_purchases\"), 2)).cache()\n",
    "\n",
    "# 7. Détermination de la catégorie la plus achetée\n",
    "# Déterminer la catégorie la plus achetée par chaque utilisateur\n",
    "category_purchases_by_user = filtered_df.filter(col(\"event_type\") == \"purchase\").groupBy(\"user_id\", \"category_code\").agg(count(\"*\").alias(\"category_purchases\")).cache()\n",
    "most_purchased_category = category_purchases_by_user.withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"category_purchases\").desc()))).filter(col(\"rank\") == 1).drop(\"rank\").cache()\n",
    "\n",
    "# 8. Calcul du temps passé sur une session et du moment le plus actif\n",
    "session_times = filtered_df.groupBy(\"user_id\", \"user_session\").agg(\n",
    "    (spark_max(\"event_time\").cast(\"long\") - spark_min(\"event_time\").cast(\"long\")).alias(\"session_duration\")\n",
    ").groupBy(\"user_id\").agg(\n",
    "    round(avg(\"session_duration\") / 60, 2).alias(\"avg_session_duration\")  # moyenne de la durée des sessions en minutes\n",
    ").cache()\n",
    "\n",
    "most_active_day = filtered_df.groupBy(\"user_id\", \"event_day_of_week\").agg(count(\"*\").alias(\"day_activity\")) \\\n",
    "                             .withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"day_activity\").desc()))) \\\n",
    "                             .filter(col(\"rank\") == 1).drop(\"rank\") \\\n",
    "                             .select(\"user_id\", col(\"event_day_of_week\").alias(\"most_active_day\")).cache()\n",
    "\n",
    "most_active_time = filtered_df.withColumn(\"time_of_day\",\n",
    "    when(col(\"event_hour\").between(5, 12), \"morning\")\n",
    "    .when(col(\"event_hour\").between(12, 17), \"afternoon\")\n",
    "    .otherwise(\"evening\")\n",
    ").groupBy(\"user_id\", \"time_of_day\").agg(count(\"*\").alias(\"time_activity\")) \\\n",
    "  .withColumn(\"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(col(\"time_activity\").desc()))) \\\n",
    "  .filter(col(\"rank\") == 1).drop(\"rank\") \\\n",
    "  .select(\"user_id\", col(\"time_of_day\").alias(\"most_active_time\")).cache()\n",
    "\n",
    "# 9. Jointure de toutes les statistiques et préparation des résultats finaux\n",
    "# Joindre toutes les statistiques\n",
    "user_stats_df = stats_all.join(days_since_last_purchase, \"user_id\") \\\n",
    "    .join(total_purchase_value, \"user_id\") \\\n",
    "    .join(cart_abandonments, \"user_id\") \\\n",
    "    .join(brand_loyalty.select(\"user_id\", \"brand_loyalty\", \"brand\"), \"user_id\", \"left\") \\\n",
    "    .join(most_purchased_category.select(\"user_id\", \"category_code\"), \"user_id\", \"left\") \\\n",
    "    .join(session_times, \"user_id\", \"left\") \\\n",
    "    .join(most_active_day, \"user_id\", \"left\") \\\n",
    "    .join(most_active_time, \"user_id\", \"left\") \\\n",
    "    .select(\"user_id\", \n",
    "            \"number_of_views_2m\", \"number_of_views_5m\", \"number_of_views_7m\", \n",
    "            \"number_of_carts_2m\", \"number_of_carts_5m\", \"number_of_carts_7m\", \n",
    "            \"number_of_sessions_2m\", \"number_of_sessions_5m\", \"number_of_sessions_7m\", \n",
    "            \"count_products_2m\", \"count_products_5m\", \"count_products_7m\", \n",
    "            \"avg_price_2m\", \"avg_price_5m\", \"avg_price_7m\", \n",
    "            \"total_purchase_value\", \"days_since_last_purchase\", \"cart_abandonments\",\n",
    "            \"brand_loyalty\", \"brand\", \"category_code\", \"avg_session_duration\",\n",
    "            \"most_active_day\", \"most_active_time\").cache()\n",
    "\n",
    "# Renommer les colonnes pour les préférences\n",
    "user_stats_df = user_stats_df.withColumnRenamed(\"brand\", \"preferred_brand\") \\\n",
    "                             .withColumnRenamed(\"category_code\", \"preferred_category\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'preferred_brand' et 'preferred_category' par des valeurs par défaut\n",
    "user_stats_df = user_stats_df.withColumn(\"preferred_brand\", coalesce(col(\"preferred_brand\"), lit(\"No Brand\"))) \\\n",
    "                             .withColumn(\"preferred_category\", coalesce(col(\"preferred_category\"), lit(\"No Category\")))\n",
    "\n",
    "# Définir des segments d'utilisateurs basés sur la valeur totale des achats\n",
    "user_stats_df = user_stats_df.withColumn(\"user_segment\", \n",
    "                                         when(col(\"total_purchase_value\") > 1000, \"High\").\n",
    "                                         when((col(\"total_purchase_value\") <= 1000) & (col(\"total_purchase_value\") > 500), \"Medium\").\n",
    "                                         otherwise(\"Low\")).cache()\n",
    "\n",
    "# Affichage du DataFrame final\n",
    "user_stats_df.show()\n",
    "\n",
    "# 10. Vérification des colonnes pour des valeurs négatives\n",
    "columns_to_check = [\n",
    "    \"number_of_views_2m\", \"number_of_views_5m\", \"number_of_views_7m\",\n",
    "    \"number_of_carts_2m\", \"number_of_carts_5m\", \"number_of_carts_7m\",\n",
    "    \"number_of_sessions_2m\", \"number_of_sessions_5m\", \"number_of_sessions_7m\",\n",
    "    \"count_products_2m\", \"count_products_5m\", \"count_products_7m\",\n",
    "    \"avg_price_2m\", \"avg_price_5m\", \"avg_price_7m\",\n",
    "    \"total_purchase_value\", \"days_since_last_purchase\", \"cart_abandonments\",\n",
    "    \"brand_loyalty\", \"avg_session_duration\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "columns_batch_1 = [\"number_of_views_2m\", \"number_of_views_5m\", \"number_of_views_7m\"]\n",
    "columns_batch_2 = [\"number_of_carts_2m\", \"number_of_carts_5m\", \"number_of_carts_7m\"]\n",
    "columns_batch_3 = [\"number_of_sessions_2m\", \"number_of_sessions_5m\", \"number_of_sessions_7m\"]\n",
    "columns_batch_4 = [\"count_products_2m\", \"count_products_5m\", \"count_products_7m\"]\n",
    "columns_batch_5 = [\"avg_price_2m\", \"avg_price_5m\", \"avg_price_7m\", \"total_purchase_value\"]\n",
    "columns_batch_6 = [\"days_since_last_purchase\", \"cart_abandonments\", \"brand_loyalty\", \"avg_session_duration\"]\n",
    "\n",
    "check_columns_for_nulls(user_stats_df, columns_batch_1)\n",
    "check_columns_for_nulls(user_stats_df, columns_batch_2)\n",
    "check_columns_for_nulls(user_stats_df, columns_batch_3)\n",
    "check_columns_for_nulls(user_stats_df, columns_batch_4)\n",
    "check_columns_for_nulls(user_stats_df, columns_batch_5)\n",
    "check_columns_for_nulls(user_stats_df, columns_batch_6)\n",
    "\n",
    "\n",
    "\n",
    "check_columns_for_negatives(user_stats_df, columns_batch_1)\n",
    "check_columns_for_negatives(user_stats_df, columns_batch_2)\n",
    "check_columns_for_negatives(user_stats_df, columns_batch_3)\n",
    "check_columns_for_negatives(user_stats_df, columns_batch_4)\n",
    "check_columns_for_negatives(user_stats_df, columns_batch_5)\n",
    "check_columns_for_negatives(user_stats_df, columns_batch_6)\n",
    "\n",
    "# 11. Enregistrement et sauvegarde de `user_stats_df`\n",
    "# Définir le chemin de sortie pour le DataFrame final\n",
    "user_stats_output_path = \"/home/jovyan/work/user_stats_df_output.parquet\"\n",
    "\n",
    "# Enregistrer et sauvegarder le DataFrame final dans un fichier Parquet\n",
    "user_stats_df.write.mode(\"overwrite\").parquet(user_stats_output_path)\n",
    "\n",
    "\n",
    "# 11. Enregistrement et sauvegarde de `user_stats_df`\n",
    "# Définir le chemin de sortie pour le DataFrame final\n",
    "# user_stats_output_path = \"/home/jovyan/work/user_stats_df_output.parquet\"\n",
    "\n",
    "# Enregistrer et sauvegarder le DataFrame final dans un fichier Parquet\n",
    "# user_stats_df.write.mode(\"overwrite\").parquet(user_stats_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec545d1-d94c-4cda-b156-d1cce262550c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
