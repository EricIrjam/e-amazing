{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f89a42-6599-494d-b142-74dc01da722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, col, split, month, year, dayofweek, hour, datediff, sum as _sum, \\\n",
    "    avg as _avg, count as _count, max as _max, current_date, round as _round, coalesce, when, lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adeb0023-4b7e-4b91-93c6-a6a7955d4f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 15:22:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Créer une session Spark avec des configurations optimisées\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-commerce Amazing Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fdd4f3-aa44-496d-a0ab-a5f818ea2eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les noms des colonnes du DataFrame final sont :\n",
      "user_id\n",
      "year\n",
      "month\n",
      "event_time\n",
      "event_type\n",
      "product_id\n",
      "category_id\n",
      "category_code\n",
      "brand\n",
      "price\n",
      "user_session\n",
      "event_day_of_week\n",
      "event_hour\n",
      "event_weekend\n",
      "price_category\n",
      "time_of_day\n",
      "number_of_views_per_month\n",
      "number_of_carts_per_month\n",
      "user_previous_purchases_per_month\n",
      "user_average_purchase_value_per_month\n",
      "last_purchase_date\n",
      "days_since_last_purchase\n",
      "cart_abandonments_per_month\n",
      "total_purchase_value_per_month\n",
      "number_of_sessions_per_month\n",
      "user_segment\n",
      "preferred_category\n",
      "preferred_brand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 15:23:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+-----------------+----------+-------------+--------------+-----------+-------------------------+-------------------------+---------------------------------+-------------------------------------+-------------------+------------------------+---------------------------+------------------------------+----------------------------+------------+--------------------+---------------+\n",
      "|  user_id|year|month|         event_time|event_type|product_id|        category_id|       category_code|  brand| price|        user_session|event_day_of_week|event_hour|event_weekend|price_category|time_of_day|number_of_views_per_month|number_of_carts_per_month|user_previous_purchases_per_month|user_average_purchase_value_per_month| last_purchase_date|days_since_last_purchase|cart_abandonments_per_month|total_purchase_value_per_month|number_of_sessions_per_month|user_segment|  preferred_category|preferred_brand|\n",
      "+---------+----+-----+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+-----------------+----------+-------------+--------------+-----------+-------------------------+-------------------------+---------------------------------+-------------------------------------+-------------------+------------------------+---------------------------+------------------------------+----------------------------+------------+--------------------+---------------+\n",
      "|478219391|2020|    4|2020-04-22 03:57:50|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|4ed4ae21-45b8-44b...|                4|         3|      weekday|          high|      night|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-22 04:22:58|      cart|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|4ed4ae21-45b8-44b...|                4|         4|      weekday|          high|      night|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-22 04:23:22|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|4ed4ae21-45b8-44b...|                4|         4|      weekday|          high|      night|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-22 04:23:43|      cart|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|4ed4ae21-45b8-44b...|                4|         4|      weekday|          high|      night|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-22 04:23:50|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|4ed4ae21-45b8-44b...|                4|         4|      weekday|          high|      night|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-22 04:25:16|      cart|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|4ed4ae21-45b8-44b...|                4|         4|      weekday|          high|      night|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-24 11:31:40|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|254.58|0edfd280-8739-47e...|                6|        11|      weekday|          high|    morning|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-24 11:32:26|      cart|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|254.58|0edfd280-8739-47e...|                6|        11|      weekday|          high|    morning|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-24 11:32:57|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|254.58|0edfd280-8739-47e...|                6|        11|      weekday|          high|    morning|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "|478219391|2020|    4|2020-04-21 12:12:33|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung|257.12|b6fae559-a164-4b6...|                3|        12|      weekday|          high|    morning|                       16|                        8|                                1|                               254.58|2020-04-24 12:47:54|                    1521|                          1|                        254.58|                          25|high spender|apparel.shoes.sli...|        samsung|\n",
      "+---------+----+-----+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+-----------------+----------+-------------+--------------+-----------+-------------------------+-------------------------+---------------------------------+-------------------------------------+-------------------+------------------------+---------------------------+------------------------------+----------------------------+------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Définir le chemin du fichier Parquet\n",
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "\n",
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Ajouter les colonnes 'month' et 'year'\n",
    "filtered_df = filtered_df.withColumn(\"month\", month(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"year\", year(col(\"event_time\")))\n",
    "\n",
    "# Filtrer les événements d'achat\n",
    "purchase_df = filtered_df.filter(col(\"event_type\") == \"purchase\")\n",
    "\n",
    "# Extraire les paires uniques category_id et category_code\n",
    "category_mapping_df = purchase_df.select(\"category_id\", \"category_code\").distinct()\n",
    "\n",
    "# Sauvegarder ce mapping dans un fichier Parquet pour une utilisation ultérieure\n",
    "mapping_output_path = \"/home/jovyan/work/category_mapping.parquet\"\n",
    "category_mapping_df.write.mode(\"overwrite\").parquet(mapping_output_path)\n",
    "\n",
    "# Charger le mapping depuis le fichier Parquet\n",
    "category_mapping_df = spark.read.parquet(mapping_output_path)\n",
    "\n",
    "# Renommer la colonne 'category_code' dans le DataFrame de mapping pour éviter l'ambiguïté\n",
    "category_mapping_df = category_mapping_df.withColumnRenamed(\"category_code\", \"mapped_category_code\")\n",
    "\n",
    "# Joindre purchase_df avec category_mapping_df pour ajouter la colonne 'mapped_category_code'\n",
    "purchase_df_with_mapping = purchase_df.join(category_mapping_df, on=\"category_id\", how=\"left\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'category_code' par les valeurs correspondantes de la jointure\n",
    "purchase_df = purchase_df_with_mapping.withColumn(\n",
    "    \"category_code\",\n",
    "    coalesce(purchase_df_with_mapping[\"category_code\"], purchase_df_with_mapping[\"mapped_category_code\"])\n",
    ")\n",
    "\n",
    "# Créer de nouvelles variables qualitatives\n",
    "filtered_df = filtered_df.withColumn(\"event_day_of_week\", dayofweek(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_hour\", hour(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_weekend\", when(col(\"event_day_of_week\").isin([1, 7]), \"weekend\").otherwise(\"weekday\")) \\\n",
    "                         .withColumn(\"price_category\", when(col(\"price\") < 50, \"low\") \\\n",
    "                                                        .when((col(\"price\") >= 50) & (col(\"price\") < 200), \"medium\") \\\n",
    "                                                        .otherwise(\"high\")) \\\n",
    "                         .withColumn(\"time_of_day\", when(col(\"event_hour\").between(0, 6), \"night\") \\\n",
    "                                                    .when(col(\"event_hour\").between(7, 12), \"morning\") \\\n",
    "                                                    .when(col(\"event_hour\").between(13, 18), \"afternoon\") \\\n",
    "                                                    .otherwise(\"evening\"))\n",
    "\n",
    "# 1. Nombre de vues par utilisateur par mois\n",
    "number_of_views_per_month = filtered_df.filter(col(\"event_type\") == \"view\") \\\n",
    "                                       .groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                       .agg(_count(\"event_type\").alias(\"number_of_views_per_month\"))\n",
    "\n",
    "# 2. Nombre de produits ajoutés au panier par utilisateur \n",
    "number_of_carts_per_month = filtered_df.filter(col(\"event_type\") == \"cart\") \\\n",
    "                                       .groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                       .agg(_count(\"event_type\").alias(\"number_of_carts_per_month\"))\n",
    "\n",
    "# 3. Nombre total d'achats précédents par utilisateur par mois\n",
    "previous_purchases_per_month = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                          .groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                          .agg(_count(\"event_type\").alias(\"user_previous_purchases_per_month\"))\n",
    "\n",
    "# 4. Valeur moyenne des achats précédents par utilisateur par mois\n",
    "average_purchase_value_per_month = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                              .groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                              .agg(_round(_avg(\"price\"), 2).alias(\"user_average_purchase_value_per_month\"))\n",
    "\n",
    "# 5. Temps écoulé depuis le dernier achat\n",
    "last_purchase_date = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                .groupBy(\"user_id\") \\\n",
    "                                .agg(_max(\"event_time\").alias(\"last_purchase_date\"))\n",
    "\n",
    "days_since_last_purchase = last_purchase_date.withColumn(\"days_since_last_purchase\", \n",
    "                                                         datediff(current_date(), col(\"last_purchase_date\")))\n",
    "\n",
    "# 6. Nombre de produits ajoutés au panier mais non achetés (abandons de panier) par mois\n",
    "cart_abandonments_per_month = filtered_df.filter(col(\"event_type\") == \"cart\") \\\n",
    "                                         .groupBy(\"user_id\", \"product_id\", \"year\", \"month\") \\\n",
    "                                         .agg(_count(\"event_type\").alias(\"cart_count\")) \\\n",
    "                                         .join(purchase_df.groupBy(\"user_id\", \"product_id\", \"year\", \"month\").agg(_count(\"event_type\").alias(\"purchase_count\")),\n",
    "                                               on=[\"user_id\", \"product_id\", \"year\", \"month\"], how=\"left\") \\\n",
    "                                         .withColumn(\"purchase_count\", col(\"purchase_count\").cast(\"int\")) \\\n",
    "                                         .na.fill(0) \\\n",
    "                                         .filter(col(\"purchase_count\") == 0) \\\n",
    "                                         .groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                         .agg(_count(\"product_id\").alias(\"cart_abandonments_per_month\"))\n",
    "\n",
    "# 7. Valeur totale des achats par utilisateur par mois\n",
    "total_purchase_value_per_month = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                            .groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                            .agg(_round(_sum(\"price\"), 2).alias(\"total_purchase_value_per_month\"))\n",
    "\n",
    "# 8. Nombre total de sessions par utilisateur par mois\n",
    "number_of_sessions_per_month = filtered_df.groupBy(\"user_id\", \"year\", \"month\") \\\n",
    "                                          .agg(_count(\"user_session\").alias(\"number_of_sessions_per_month\"))\n",
    "\n",
    "# Calculer la valeur moyenne des achats pour déterminer le segment utilisateur\n",
    "average_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                    .groupBy(\"user_id\") \\\n",
    "                                    .agg(_avg(\"price\").alias(\"avg_purchase_value\"))\n",
    "\n",
    "average_purchase_value = average_purchase_value.withColumn(\"user_segment\", when(col(\"avg_purchase_value\") >= 100, \"high spender\").otherwise(\"regular buyer\"))\n",
    "\n",
    "# Identifier la catégorie de produit préférée de l'utilisateur\n",
    "preferred_category = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                .groupBy(\"user_id\", \"category_code\") \\\n",
    "                                .agg(_count(\"category_code\").alias(\"category_count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"category_count\").desc())\n",
    "preferred_category = preferred_category.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                                       .filter(col(\"rank\") == 1) \\\n",
    "                                       .select(\"user_id\", \"category_code\") \\\n",
    "                                       .withColumnRenamed(\"category_code\", \"preferred_category\")\n",
    "\n",
    "# Indicateur de fidélité à une marque spécifique\n",
    "brand_loyalty = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                           .groupBy(\"user_id\", \"brand\") \\\n",
    "                           .agg(_count(\"brand\").alias(\"brand_count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"brand_count\").desc())\n",
    "brand_loyalty = brand_loyalty.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                             .filter(col(\"rank\") == 1) \\\n",
    "                             .select(\"user_id\", \"brand\") \\\n",
    "                             .withColumnRenamed(\"brand\", \"preferred_brand\")\n",
    "\n",
    "# Joindre les DataFrames pour créer le DataFrame final\n",
    "final_df = filtered_df.join(number_of_views_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(number_of_carts_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(previous_purchases_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(average_purchase_value_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(days_since_last_purchase, \"user_id\", \"left\") \\\n",
    "                      .join(cart_abandonments_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(total_purchase_value_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(number_of_sessions_per_month, [\"user_id\", \"year\", \"month\"], \"left\") \\\n",
    "                      .join(average_purchase_value.select(\"user_id\", \"user_segment\"), on=\"user_id\", how=\"left\") \\\n",
    "                      .join(preferred_category, on=\"user_id\", how=\"left\") \\\n",
    "                      .join(brand_loyalty, on=\"user_id\", how=\"left\") \\\n",
    "                      .distinct()  # Suppression des duplicatas si nécessaire\n",
    "\n",
    "# Remplacer les valeurs NULL par des valeurs par défaut si nécessaire\n",
    "final_df = final_df.fillna({\n",
    "    \"number_of_views_per_month\": 0,\n",
    "    \"number_of_carts_per_month\": 0,\n",
    "    \"user_previous_purchases_per_month\": 0,\n",
    "    \"user_average_purchase_value_per_month\": 0.0,\n",
    "    \"days_since_last_purchase\": 9999,  # Utiliser une valeur par défaut pour indiquer une absence de précédent achat\n",
    "    \"cart_abandonments_per_month\": 0,\n",
    "    \"total_purchase_value_per_month\": 0.0,\n",
    "    \"number_of_sessions_per_month\": 0\n",
    "})\n",
    "\n",
    "# Afficher les noms des colonnes pour vérifier les nouvelles variables\n",
    "print(\"Les noms des colonnes du DataFrame final sont :\")\n",
    "for column in final_df.columns:\n",
    "    print(column)\n",
    "\n",
    "# Afficher un échantillon des données pour vérifier les nouvelles variables\n",
    "final_df.show(10)\n",
    "\n",
    "# Sauvegarder le DataFrame final dans un fichier Parquet\n",
    "final_output_path = \"/home/jovyan/work/final_df_output.parquet\"\n",
    "final_df.write.mode(\"overwrite\").parquet(final_output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89ca995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.+ 0) / 2][Stage 168:>  (0 + 0) / 2]1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o408.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Afficher un échantillon des données pour vérifier les nouvelles variables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o408.showString"
     ]
    }
   ],
   "source": [
    "# Afficher un échantillon des données pour vérifier les nouvelles variables\n",
    "final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eaf7ec-b831-49d4-97bc-d960a139620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Arrêter la session Spark\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
