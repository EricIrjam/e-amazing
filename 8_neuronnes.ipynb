{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Charger le jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prétraitement des données\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Construction de l'Autoencoder\n",
    "\n",
    "Un autoencoder est un type de réseau de neurones qui apprend à compresser les données dans une représentation latente de taille réduite (appelé \"code\"), puis à les reconstruire à partir de ce code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Encoder les données\n",
    "\n",
    "Après l'entraînement de l'autoencoder, nous utilisons uniquement la partie \"encoder\" du modèle pour obtenir les caractéristiques réduites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clustering sur les données encodées\n",
    "\n",
    "Maintenant que les données ont été réduites à un espace de plus faible dimension, nous appliquons un algorithme de clustering comme K-Means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualisation des clusters (optionnel)\n",
    "\n",
    "Si tes données ont une faible dimension (par exemple, 2 ou 3 dimensions après encodage), tu peux visualiser les clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Sauvegarde des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé\n",
    "\n",
    "Ce script :\n",
    "1. Charge et normalise les données.\n",
    "2. Utilise un autoencoder pour réduire la dimension des données.\n",
    "3. Applique K-Means sur les données encodées pour identifier les clusters.\n",
    "4. Visualise et sauvegarde les résultats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Améliorer dernier version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers, losses\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "2122/2122 [==============================] - 21s 9ms/step - loss: 0.0820 - val_loss: 0.0327 - lr: 4.7580e-04\n",
      "Epoch 2/17\n",
      "2115/2122 [============================>.] - ETA: 0s - loss: 0.0400"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Construction du modèle autoencodeur\n",
    "input_layer = layers.Input(shape=(df_pca.shape[1],))\n",
    "encoded = layers.Dense(16, activation='leaky_relu', activity_regularizer=regularizers.l2(1e-4))(input_layer)\n",
    "encoded = layers.GaussianNoise(0.1)(encoded)  # Ajout de bruit pour introduire de la variabilité\n",
    "encoded = layers.Dropout(0.5)(encoded)\n",
    "encoded = layers.Dense(10, activation='leaky_relu')(encoded)\n",
    "decoded = layers.Dense(16, activation='leaky_relu')(encoded)\n",
    "decoded = layers.Dense(df_pca.shape[1], activation='sigmoid')(decoded)\n",
    "\n",
    "# Modèle autoencodeur\n",
    "autoencoder = models.Model(input_layer, decoded)\n",
    "\n",
    "# Cyclical Learning Rate pour ajustement dynamique du taux d'apprentissage\n",
    "initial_lr = 1e-4\n",
    "lr_schedule = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    maximal_learning_rate=5e-4,\n",
    "    scale_fn=lambda x: 1 / (2.0 ** (x - 1)),\n",
    "    step_size=2000\n",
    ")\n",
    "\n",
    "# Compilation du modèle avec MSLE pour capturer les petites erreurs\n",
    "autoencoder.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss=losses.MeanSquaredLogarithmicError())\n",
    "\n",
    "# Callbacks pour ajuster l'apprentissage et stopper tôt si nécessaire\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = autoencoder.fit(\n",
    "    df_pca, df_pca,\n",
    "    epochs=17,  # Utiliser EarlyStopping pour ajuster dynamiquement\n",
    "    batch_size=32, shuffle=True, validation_split=0.2,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Affichage des courbes de perte pour évaluer l'apprentissage\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Courbes de Perte d’Entraînement et de Validation')\n",
    "plt.show()\n",
    "\n",
    "# Définir le modèle encodeur à partir de l'autoencodeur\n",
    "encoder = models.Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Encoder les données pour obtenir les représentations latentes\n",
    "encoded_data = encoder.predict(df_pca)\n",
    "\n",
    "# Appliquer K-Means sur les représentations encodées pour le clustering\n",
    "kmeans = KMeans(n_clusters=6, n_init=10)\n",
    "clusters = kmeans.fit_predict(encoded_data)\n",
    "\n",
    "# Réduction dimensionnelle avec PCA pour la visualisation\n",
    "pca_result = PCA(n_components=2).fit_transform(encoded_data)\n",
    "\n",
    "# Ajouter les résultats au DataFrame pour la visualisation des clusters\n",
    "df['pca1'], df['pca2'] = pca_result[:, 0], pca_result[:, 1]\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# Visualisation des clusters après réduction dimensionnelle\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='pca1', y='pca2', hue='cluster', data=df, palette='Set2', legend='full')\n",
    "plt.title('Visualisation des clusters après réduction dimensionnelle')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.58\n",
      "Calinski-Harabasz Index: 20130.72\n",
      "Davies-Bouldin Index: 1.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Calculer les métriques de segmentation\n",
    "silhouette_avg = silhouette_score(encoded_data, clusters)\n",
    "calinski_harabasz = calinski_harabasz_score(encoded_data, clusters)\n",
    "davies_bouldin = davies_bouldin_score(encoded_data, clusters)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.2f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Interprétation des Résultats de Segmentation :\n",
    "Silhouette Score : Plus proche de +1 est idéal, indiquant des clusters bien définis.\n",
    "\n",
    "Calinski-Harabasz Index : Des valeurs plus élevées suggèrent une meilleure séparation.\n",
    "\n",
    "Davies-Bouldin Index : Plus faible est meilleur, indiquant des clusters bien séparés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
