{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33727 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 19:10:59,828 - distributed.scheduler - WARNING - Worker failed to heartbeat for 561s; attempting restart: <WorkerState 'tcp://127.0.0.1:39745', name: 0, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 19:10:59,832 - distributed.scheduler - WARNING - Worker failed to heartbeat for 561s; attempting restart: <WorkerState 'tcp://127.0.0.1:40279', name: 3, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 19:11:04,797 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 19:11:04,815 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 19:11:05,805 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 19:11:06,629 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 19:18:44,328 - distributed.scheduler - WARNING - Worker failed to heartbeat for 417s; attempting restart: <WorkerState 'tcp://127.0.0.1:37099', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 19:18:44,341 - distributed.scheduler - WARNING - Worker failed to heartbeat for 417s; attempting restart: <WorkerState 'tcp://127.0.0.1:43695', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 19:18:50,493 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 19:18:51,049 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 19:18:51,112 - distributed.core - ERROR - Exception while handling op kill\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\n",
      "    fut.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 829, in _handle_comm\n",
      "    result = await result\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/nanny.py\", line 400, in kill\n",
      "    await self.process.kill(reason=reason, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/nanny.py\", line 875, in kill\n",
      "    await process.join(max(0, deadline - time()))\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/process.py\", line 330, in join\n",
      "    await wait_for(asyncio.shield(self._exit_future), timeout)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/usr/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "2024-08-29 19:18:51,725 - distributed.core - ERROR - Exception while handling op kill\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/asyncio/tasks.py\", line 452, in wait_for\n",
      "    fut.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 829, in _handle_comm\n",
      "    result = await result\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/nanny.py\", line 400, in kill\n",
      "    await self.process.kill(reason=reason, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/nanny.py\", line 875, in kill\n",
      "    await process.join(max(0, deadline - time()))\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/process.py\", line 330, in join\n",
      "    await wait_for(asyncio.shield(self._exit_future), timeout)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/usr/lib/python3.9/asyncio/tasks.py\", line 454, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "2024-08-29 19:18:52,029 - distributed.scheduler - ERROR - Workers ['tcp://127.0.0.1:43695', 'tcp://127.0.0.1:37099'] did not shut down within 30s; force closing\n",
      "2024-08-29 19:18:52,279 - distributed.scheduler - ERROR - 2/2 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:43695', 'tcp://127.0.0.1:37099'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 837, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/scheduler.py\", line 6563, in restart_workers\n",
      "    raise TimeoutError(\n",
      "asyncio.exceptions.TimeoutError: 2/2 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:43695', 'tcp://127.0.0.1:37099'}\n",
      "2024-08-29 19:18:52,284 - distributed.scheduler - ERROR - 2/2 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:43695', 'tcp://127.0.0.1:37099'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 837, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/scheduler.py\", line 8609, in check_worker_ttl\n",
      "    await self.restart_workers(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 837, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/scheduler.py\", line 6563, in restart_workers\n",
      "    raise TimeoutError(\n",
      "asyncio.exceptions.TimeoutError: 2/2 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:43695', 'tcp://127.0.0.1:37099'}\n",
      "2024-08-29 19:18:52,332 - tornado.application - ERROR - Exception in callback <bound method Scheduler.check_worker_ttl of <Scheduler 'tcp://127.0.0.1:39907', workers: 0, cores: 0, tasks: 0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 939, in _run\n",
      "    await val\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 837, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/scheduler.py\", line 8609, in check_worker_ttl\n",
      "    await self.restart_workers(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils.py\", line 837, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/scheduler.py\", line 6563, in restart_workers\n",
      "    raise TimeoutError(\n",
      "asyncio.exceptions.TimeoutError: 2/2 nanny worker(s) did not shut down within 30s: {'tcp://127.0.0.1:43695', 'tcp://127.0.0.1:37099'}\n",
      "2024-08-29 19:18:52,342 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 19:18:52,545 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 19:34:30,880 - distributed.scheduler - WARNING - Worker failed to heartbeat for 340s; attempting restart: <WorkerState 'tcp://127.0.0.1:38685', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 19:34:31,145 - distributed.scheduler - WARNING - Worker failed to heartbeat for 340s; attempting restart: <WorkerState 'tcp://127.0.0.1:41069', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 19:34:37,626 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 19:34:37,825 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 19:34:38,151 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 19:34:38,332 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 20:37:42,320 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1795s; attempting restart: <WorkerState 'tcp://127.0.0.1:40007', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 20:37:47,608 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 20:55:11,608 - distributed.scheduler - WARNING - Worker failed to heartbeat for 559s; attempting restart: <WorkerState 'tcp://127.0.0.1:44275', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 20:55:12,981 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 21:15:27,452 - distributed.scheduler - WARNING - Worker failed to heartbeat for 559s; attempting restart: <WorkerState 'tcp://127.0.0.1:38515', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 21:15:28,781 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "# Initialiser Dask\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données avec Dask\n",
    "output_path = \"./data/filtered_df_output.parquet\"\n",
    "df = dd.read_parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les heures en périodes de la journée\n",
    "def time_of_day(hour):\n",
    "    if 5 <= hour <= 11:\n",
    "        return 'matin'\n",
    "    elif 12 <= hour <= 16:\n",
    "        return 'après-midi'\n",
    "    elif 17 <= hour <= 23:\n",
    "        return 'soir'\n",
    "    else:\n",
    "        return 'nuit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne pour extraire l'heure de l'événement\n",
    "df['event_hour'] = df['event_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Appliquer la fonction sur toute la colonne\n",
    "df['most_active_time'] = df['event_hour'].apply(time_of_day, meta=('event_hour', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer les statistiques pour une période donnée\n",
    "def compute_user_stats(df, period_label, period_offset):\n",
    "    period_df = df[df['event_time'] >= df['event_time'].max() - pd.DateOffset(months=period_offset)]\n",
    "\n",
    "    # Calculer les vues, les paniers et les achats\n",
    "    number_of_views = period_df[period_df['event_type'] == 'view'].groupby('user_id').size().rename(f'number_of_views_{period_label}')\n",
    "    number_of_carts = period_df[period_df['event_type'] == 'cart'].groupby('user_id').size().rename(f'number_of_carts_{period_label}')\n",
    "    count_products = period_df[period_df['event_type'] == 'purchase'].groupby('user_id').size().rename(f'count_products_{period_label}')\n",
    "\n",
    "    # Calculer le prix moyen des achats\n",
    "    avg_price = period_df[period_df['event_type'] == 'purchase'].groupby('user_id')['price'].mean().rename(f'avg_price_{period_label}')\n",
    "\n",
    "    # Calculer le nombre de sessions\n",
    "    number_of_sessions = period_df.groupby('user_id')['user_session'].nunique().rename(f'number_of_sessions_{period_label}')\n",
    "\n",
    "    # Fusionner toutes les statistiques\n",
    "    stats = dd.concat([number_of_views, number_of_carts, count_products, avg_price, number_of_sessions], axis=1)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques pour les périodes de 2, 5, et 7 mois\n",
    "stats_2m = compute_user_stats(df, '2m', 2)\n",
    "stats_5m = compute_user_stats(df, '5m', 5)\n",
    "stats_7m = compute_user_stats(df, '7m', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner toutes les statistiques ensemble\n",
    "user_stats_df = stats_2m.merge(stats_5m, on='user_id', how='outer').merge(stats_7m, on='user_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculer les statistiques supplémentaires\n",
    "last_purchase = df[df['event_type'] == 'purchase'].groupby('user_id')['event_time'].max()\n",
    "days_since_last_purchase = (df['event_time'].max() - last_purchase).dt.days\n",
    "total_purchase_value = df[df['event_type'] == 'purchase'].groupby('user_id')['price'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les statistiques supplémentaires au DataFrame\n",
    "user_stats_df['last_purchase_temp'] = last_purchase\n",
    "user_stats_df['days_since_last_purchase'] = days_since_last_purchase\n",
    "user_stats_df['total_purchase_value'] = total_purchase_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les abandons de panier\n",
    "cart_count = df[df['event_type'] == 'cart'].groupby('user_id').size()\n",
    "purchase_count = df[df['event_type'] == 'purchase'].groupby('user_id').size()\n",
    "cart_abandonments = cart_count - purchase_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utiliser .where pour remplacer les valeurs négatives par 0\n",
    "cart_abandonments = cart_abandonments.where(cart_abandonments >= 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats_df['cart_count'] = cart_count\n",
    "user_stats_df['purchase_count'] = purchase_count\n",
    "user_stats_df['cart_abandonments'] = cart_abandonments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la partie nécessaire en Pandas pour calculer les préférences de marque et de catégorie\n",
    "df_pandas = df[df['event_type'] == 'purchase'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les préférences de marque\n",
    "preferred_brand = df_pandas.groupby(['user_id', 'brand']).size().reset_index()\n",
    "preferred_brand.columns = ['user_id', 'brand', 'count']  # Renommer les colonnes correctement\n",
    "preferred_brand = preferred_brand.loc[preferred_brand.groupby('user_id')['count'].idxmax()].set_index('user_id')['brand']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les préférences de catégorie\n",
    "preferred_category = df_pandas.groupby(['user_id', 'category_code']).size().reset_index()\n",
    "preferred_category.columns = ['user_id', 'category_code', 'count']  # Renommer les colonnes correctement\n",
    "preferred_category = preferred_category.loc[preferred_category.groupby('user_id')['count'].idxmax()].set_index('user_id')['category_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajouter les préférences au DataFrame principal\n",
    "user_stats_df['preferred_brand'] = preferred_brand\n",
    "user_stats_df['preferred_category'] = preferred_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la période la plus active pour chaque utilisateur\n",
    "most_active_time_df = df.groupby(['user_id', 'most_active_time']).size().reset_index()\n",
    "most_active_time_df.columns = ['user_id', 'most_active_time', 'activity_count']  # Renommer les colonnes correctement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'compute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1383/1129436404.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;31m# Convertir cette partie en Pandas pour faire le rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmost_active_time_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_active_time_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmost_active_time_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_active_time_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activity_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'compute'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 20:37:45,159 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:52556 remote=tcp://127.0.0.1:36879>: Stream is closed\n",
      "2024-08-29 20:37:45,178 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:51290 remote=tcp://127.0.0.1:36879>: Stream is closed\n",
      "2024-08-29 20:37:45,283 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:47092 remote=tcp://127.0.0.1:40587>: Stream is closed\n",
      "2024-08-29 20:55:12,019 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:44234 remote=tcp://127.0.0.1:42559>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "# Convertir cette partie en Pandas pour faire le rank\n",
    "most_active_time_df = most_active_time_df.compute()\n",
    "most_active_time_df['rank'] = most_active_time_df.groupby('user_id')['activity_count'].rank(method='first', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour ne garder que la période la plus active\n",
    "most_active_time_df = most_active_time_df[most_active_time_df['rank'] == 1].drop(columns=['rank', 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la colonne 'most_active_time' s'il existe déjà dans user_stats_df\n",
    "if 'most_active_time' in user_stats_df.columns:\n",
    "    user_stats_df = user_stats_df.drop(columns=['most_active_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réaliser la jointure\n",
    "user_stats_df = user_stats_df.merge(most_active_time_df[['user_id', 'most_active_time']], on='user_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vérification des colonnes\n",
    "print(user_stats_df.columns)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(user_stats_df.head())\n",
    "\n",
    "# Optionnel : Sauvegarder le DataFrame final\n",
    "user_stats_df.to_parquet('./data/user_stats_df_final.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41179 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reset_index() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m user_stats_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreferred_category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preferred_category\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Calculer la période la plus active pour chaque utilisateur\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m most_active_time_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmost_active_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactivity_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Convertir cette partie en Pandas pour faire le rank\u001b[39;00m\n\u001b[1;32m     99\u001b[0m most_active_time_df \u001b[38;5;241m=\u001b[39m most_active_time_df\u001b[38;5;241m.\u001b[39mcompute()  \u001b[38;5;66;03m# C'est déjà un DataFrame Pandas maintenant\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: reset_index() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "\n",
    "# Initialiser Dask\n",
    "client = Client()\n",
    "\n",
    "# Charger les données avec Dask\n",
    "output_path = \"./data/filtered_df_output.parquet\"\n",
    "df = dd.read_parquet(output_path)\n",
    "\n",
    "# Convertir les heures en périodes de la journée\n",
    "def time_of_day(hour):\n",
    "    if 5 <= hour <= 11:\n",
    "        return 'matin'\n",
    "    elif 12 <= hour <= 16:\n",
    "        return 'après-midi'\n",
    "    elif 17 <= hour <= 23:\n",
    "        return 'soir'\n",
    "    else:\n",
    "        return 'nuit'\n",
    "\n",
    "# Ajouter une colonne pour extraire l'heure de l'événement\n",
    "df['event_hour'] = df['event_time'].dt.hour\n",
    "\n",
    "# Appliquer la fonction sur toute la colonne\n",
    "df['most_active_time'] = df['event_hour'].apply(time_of_day, meta=('event_hour', 'object'))\n",
    "\n",
    "# Fonction pour calculer les statistiques pour une période donnée\n",
    "def compute_user_stats(df, period_label, period_offset):\n",
    "    period_df = df[df['event_time'] >= df['event_time'].max() - pd.DateOffset(months=period_offset)]\n",
    "    \n",
    "    # Calculer les vues, les paniers et les achats\n",
    "    number_of_views = period_df[period_df['event_type'] == 'view'].groupby('user_id').size().rename(f'number_of_views_{period_label}')\n",
    "    number_of_carts = period_df[period_df['event_type'] == 'cart'].groupby('user_id').size().rename(f'number_of_carts_{period_label}')\n",
    "    count_products = period_df[period_df['event_type'] == 'purchase'].groupby('user_id').size().rename(f'count_products_{period_label}')\n",
    "    \n",
    "    # Calculer le prix moyen des achats\n",
    "    avg_price = period_df[period_df['event_type'] == 'purchase'].groupby('user_id')['price'].mean().rename(f'avg_price_{period_label}')\n",
    "    \n",
    "    # Calculer le nombre de sessions\n",
    "    number_of_sessions = period_df.groupby('user_id')['user_session'].nunique().rename(f'number_of_sessions_{period_label}')\n",
    "    \n",
    "    # Fusionner toutes les statistiques\n",
    "    stats = dd.concat([number_of_views, number_of_carts, count_products, avg_price, number_of_sessions], axis=1)\n",
    "    return stats\n",
    "\n",
    "# Calculer les statistiques pour les périodes de 2, 5, et 7 mois\n",
    "stats_2m = compute_user_stats(df, '2m', 2)\n",
    "stats_5m = compute_user_stats(df, '5m', 5)\n",
    "stats_7m = compute_user_stats(df, '7m', 7)\n",
    "\n",
    "# Fusionner toutes les statistiques ensemble\n",
    "user_stats_df = stats_2m.merge(stats_5m, on='user_id', how='outer').merge(stats_7m, on='user_id', how='outer')\n",
    "\n",
    "# Calculer les statistiques supplémentaires\n",
    "last_purchase = df[df['event_type'] == 'purchase'].groupby('user_id')['event_time'].max()\n",
    "days_since_last_purchase = (df['event_time'].max() - last_purchase).dt.days\n",
    "total_purchase_value = df[df['event_type'] == 'purchase'].groupby('user_id')['price'].sum()\n",
    "\n",
    "# Ajouter les statistiques supplémentaires au DataFrame\n",
    "user_stats_df['last_purchase_temp'] = last_purchase\n",
    "user_stats_df['days_since_last_purchase'] = days_since_last_purchase\n",
    "user_stats_df['total_purchase_value'] = total_purchase_value\n",
    "\n",
    "# Calculer les abandons de panier\n",
    "cart_count = df[df['event_type'] == 'cart'].groupby('user_id').size()\n",
    "purchase_count = df[df['event_type'] == 'purchase'].groupby('user_id').size()\n",
    "cart_abandonments = cart_count - purchase_count\n",
    "\n",
    "# Utiliser .where pour remplacer les valeurs négatives par 0\n",
    "cart_abandonments = cart_abandonments.where(cart_abandonments >= 0, 0)\n",
    "\n",
    "user_stats_df['cart_count'] = cart_count\n",
    "user_stats_df['purchase_count'] = purchase_count\n",
    "user_stats_df['cart_abandonments'] = cart_abandonments\n",
    "\n",
    "# Convertir la partie nécessaire en Pandas pour calculer les préférences de marque et de catégorie\n",
    "df_pandas = df[df['event_type'] == 'purchase'].compute()\n",
    "\n",
    "# Calculer les préférences de marque\n",
    "preferred_brand = df_pandas.groupby(['user_id', 'brand']).size().reset_index()\n",
    "preferred_brand.columns = ['user_id', 'brand', 'count']  # Renommer les colonnes correctement\n",
    "preferred_brand = preferred_brand.loc[preferred_brand.groupby('user_id')['count'].idxmax()].set_index('user_id')['brand']\n",
    "\n",
    "# Calculer les préférences de catégorie\n",
    "preferred_category = df_pandas.groupby(['user_id', 'category_code']).size().reset_index()\n",
    "preferred_category.columns = ['user_id', 'category_code', 'count']  # Renommer les colonnes correctement\n",
    "preferred_category = preferred_category.loc[preferred_category.groupby('user_id')['count'].idxmax()].set_index('user_id')['category_code']\n",
    "\n",
    "# Ajouter les préférences au DataFrame principal\n",
    "user_stats_df['preferred_brand'] = preferred_brand\n",
    "user_stats_df['preferred_category'] = preferred_category\n",
    "\n",
    "# Calculer la période la plus active pour chaque utilisateur\n",
    "most_active_time_df = df.groupby(['user_id', 'most_active_time']).size().reset_index(name='activity_count')\n",
    "\n",
    "# Convertir cette partie en Pandas pour faire le rank\n",
    "most_active_time_df = most_active_time_df.compute()  # C'est déjà un DataFrame Pandas maintenant\n",
    "most_active_time_df['rank'] = most_active_time_df.groupby('user_id')['activity_count'].rank(method='first', ascending=False)\n",
    "\n",
    "# Filtrer pour ne garder que la période la plus active\n",
    "most_active_time_df = most_active_time_df[most_active_time_df['rank'] == 1].drop(columns=['rank', 'activity_count'])\n",
    "\n",
    "# Supprimer la colonne 'most_active_time' s'il existe déjà dans user_stats_df\n",
    "if 'most_active_time' in user_stats_df.columns:\n",
    "    user_stats_df = user_stats_df.drop(columns=['most_active_time'])\n",
    "\n",
    "# Réaliser la jointure\n",
    "user_stats_df = user_stats_df.merge(most_active_time_df[['user_id', 'most_active_time']], on='user_id', how='inner')\n",
    "\n",
    "# Vérification des colonnes\n",
    "print(user_stats_df.columns)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(user_stats_df.head().compute())  # Utilisez compute() avant d'afficher les résultats\n",
    "\n",
    "# Optionnel : Sauvegarder le DataFrame final\n",
    "user_stats_df.to_parquet('./data/user_stats_df_final.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35199 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'number_of_views_2m', 'number_of_carts_2m',\n",
      "       'count_products_2m', 'avg_price_2m', 'number_of_sessions_2m',\n",
      "       'number_of_views_5m', 'number_of_carts_5m', 'count_products_5m',\n",
      "       'avg_price_5m', 'number_of_sessions_5m', 'number_of_views_7m',\n",
      "       'number_of_carts_7m', 'count_products_7m', 'avg_price_7m',\n",
      "       'number_of_sessions_7m', 'last_purchase_temp',\n",
      "       'days_since_last_purchase', 'total_purchase_value', 'cart_count',\n",
      "       'purchase_count', 'cart_abandonments', 'preferred_brand',\n",
      "       'preferred_category', 'most_active_time'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(user_stats_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Afficher les premières lignes\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43muser_stats_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# Utilisez compute() avant d'afficher les résultats\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Optionnel : Sauvegarder le DataFrame final\u001b[39;00m\n\u001b[1;32m    122\u001b[0m user_stats_df\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/user_stats_df_final.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, Scalar):\n\u001b[1;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 476\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_collection.py:591\u001b[0m, in \u001b[0;36mFrameBase.optimize\u001b[0;34m(self, fuse)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    574\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimizes the DataFrame.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m    Runs the optimizer with all steps over the DataFrame and wraps the result in a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m        The optimized Dask Dataframe\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:94\u001b[0m, in \u001b[0;36mExpr.optimize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3063\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(expr, fuse)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"High level query optimization\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m \n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03mThis leverages three optimization passes:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;124;03moptimize_blockwise_fusion\u001b[39;00m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m stage: core\u001b[38;5;241m.\u001b[39mOptimizerStage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified-physical\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize_until\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3024\u001b[0m, in \u001b[0;36moptimize_until\u001b[0;34m(expr, stage)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# Lower\u001b[39;00m\n\u001b[0;32m-> 3024\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_completely\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphysical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_core.py:447\u001b[0m, in \u001b[0;36mExpr.lower_completely\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m lowered \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlowered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m==\u001b[39m expr\u001b[38;5;241m.\u001b[39m_name:\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_core.py:402\u001b[0m, in \u001b[0;36mExpr.lower_once\u001b[0;34m(self, lowered)\u001b[0m\n\u001b[1;32m    399\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Lower this node\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     out \u001b[38;5;241m=\u001b[39m expr\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_repartition.py:81\u001b[0m, in \u001b[0;36mRepartition._lower\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_partitions \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RepartitionToFewer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_partitions \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mnpartitions:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# Remove if partitions are equal\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_merge.py:219\u001b[0m, in \u001b[0;36mMerge._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_indexed_left\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_indexed_right:\n\u001b[1;32m    220\u001b[0m         divisions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    221\u001b[0m             unique(merge_sorted(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39mdivisions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mdivisions))\n\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(divisions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_merge.py:323\u001b[0m, in \u001b[0;36mMerge.merge_indexed_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_indexed_left\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_index \u001b[38;5;129;01mor\u001b[39;00m _contains_index_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on)\n\u001b[0;32m--> 323\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown_divisions\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:390\u001b[0m, in \u001b[0;36mExpr.known_divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknown_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Whether divisions are already known\"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3405\u001b[0m, in \u001b[0;36mMaybeAlignPartitions._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m {df\u001b[38;5;241m.\u001b[39mnpartitions \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs} \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m   3406\u001b[0m         divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3405\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m {\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs} \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m   3406\u001b[0m         divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3405\u001b[0m, in \u001b[0;36mMaybeAlignPartitions._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m {df\u001b[38;5;241m.\u001b[39mnpartitions \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs} \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m   3406\u001b[0m         divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3405\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m {\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs} \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m   3406\u001b[0m         divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: cached_property.__get__ at line 969 (4 times), MaybeAlignPartitions._divisions at line 3405 (4 times), Expr.divisions at line 382 (4 times), <setcomp> at line 3405 (3 times), Expr.npartitions at line 398 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3405\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m {\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs} \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m   3406\u001b[0m         divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: cached_property.__get__ at line 969 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3405\u001b[0m, in \u001b[0;36mMaybeAlignPartitions._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m {df\u001b[38;5;241m.\u001b[39mnpartitions \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m} \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m1\u001b[39m}:\n\u001b[1;32m   3406\u001b[0m         divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3423\u001b[0m, in \u001b[0;36mMaybeAlignPartitions.args\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3420\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m   3421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3422\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, Expr)]\n\u001b[0;32m-> 3423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m dfs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_broadcastable(dfs, op)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3423\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3420\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m   3421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3422\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, Expr)]\n\u001b[0;32m-> 3423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m dfs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_broadcastable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:3079\u001b[0m, in \u001b[0;36mis_broadcastable\u001b[0;34m(dfs, s)\u001b[0m\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   3075\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   3078\u001b[0m     s\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 3079\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3080\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39mknown_divisions\n\u001b[1;32m   3081\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(compare(s, df) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dfs \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   3082\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m s\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3083\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:529\u001b[0m, in \u001b[0;36mBlockwise._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies()\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m dependencies:\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_dep\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mdivisions \u001b[38;5;241m==\u001b[39m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:520\u001b[0m, in \u001b[0;36mBlockwise._broadcast_dep\u001b[0;34m(self, dep)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_dep\u001b[39m(\u001b[38;5;28mself\u001b[39m, dep: Expr):\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# Checks if a dependency should be broadcasted to\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# all partitions of this `Blockwise` operation\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dep\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/functools.py:969\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    967\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 969\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    971\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:2626\u001b[0m, in \u001b[0;36mBinop._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(left_divisions, right_divisions))\n\u001b[1;32m   2625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_expr.py:530\u001b[0m, in \u001b[0;36mBlockwise._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m dependencies:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broadcast_dep(arg):\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mdivisions \u001b[38;5;241m==\u001b[39m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 20:37:42,351 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1795s; attempting restart: <WorkerState 'tcp://127.0.0.1:34079', name: 0, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 20:37:42,483 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1795s; attempting restart: <WorkerState 'tcp://127.0.0.1:35179', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 20:37:42,670 - distributed.scheduler - WARNING - Worker failed to heartbeat for 1795s; attempting restart: <WorkerState 'tcp://127.0.0.1:35811', name: 3, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 20:37:49,687 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 20:37:49,696 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 20:37:50,041 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 20:38:01,427 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 20:38:01,563 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 21:15:27,538 - distributed.scheduler - WARNING - Worker failed to heartbeat for 559s; attempting restart: <WorkerState 'tcp://127.0.0.1:43809', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 21:15:29,453 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 21:25:35,204 - distributed.scheduler - WARNING - Worker failed to heartbeat for 559s; attempting restart: <WorkerState 'tcp://127.0.0.1:39319', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 21:25:36,209 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-08-29 21:52:54,517 - distributed.scheduler - WARNING - Worker failed to heartbeat for 557s; attempting restart: <WorkerState 'tcp://127.0.0.1:33917', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-08-29 21:52:55,575 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41951 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Column not found: rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_groupby.py:1593\u001b[0m, in \u001b[0;36mGroupBy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_groupby.py:1615\u001b[0m, in \u001b[0;36mGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key):\n\u001b[0;32m-> 1615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m g \u001b[38;5;241m=\u001b[39m GroupBy(\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj,\n\u001b[1;32m   1625\u001b[0m     by\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys,\n\u001b[1;32m   1631\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_groupby.py:2214\u001b[0m, in \u001b[0;36mSeriesGroupBy.__init__\u001b[0;34m(self, obj, by, sort, observed, dropna, slice)\u001b[0m\n\u001b[1;32m   2212\u001b[0m         obj\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mgroupby(by, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_as_dict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m\"\u001b[39m, observed))\n\u001b[0;32m-> 2214\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_groupby.py:1558\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, by, group_keys, sort, observed, dropna, slice)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mslice\u001b[39m)\n\u001b[0;32m-> 1558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/groupby/generic.py:1951\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1950\u001b[0m     )\n\u001b[0;32m-> 1951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: rank'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m most_active_time_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost_active_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_count\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Renommer la colonne de taille pour éviter les erreurs\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Calculer le rang de l'activité\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m most_active_time_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmost_active_time_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactivity_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Filtrer pour ne garder que la période la plus active\u001b[39;00m\n\u001b[1;32m    103\u001b[0m most_active_time_df \u001b[38;5;241m=\u001b[39m most_active_time_df[most_active_time_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_count\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/dask_expr/_groupby.py:1595\u001b[0m, in \u001b[0;36mGroupBy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1595\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Column not found: rank'"
     ]
    }
   ],
   "source": [
    "# import dask.dataframe as dd\n",
    "# from dask.distributed import Client\n",
    "# import pandas as pd\n",
    "\n",
    "# # Initialiser Dask\n",
    "# client = Client()\n",
    "\n",
    "# # Charger les données avec Dask\n",
    "# output_path = \"./data/filtered_df_output.parquet\"\n",
    "# df = dd.read_parquet(output_path)\n",
    "\n",
    "# # Convertir les heures en périodes de la journée\n",
    "# def time_of_day(hour):\n",
    "#     if 5 <= hour <= 11:\n",
    "#         return 'matin'\n",
    "#     elif 12 <= hour <= 16:\n",
    "#         return 'après-midi'\n",
    "#     elif 17 <= hour <= 23:\n",
    "#         return 'soir'\n",
    "#     else:\n",
    "#         return 'nuit'\n",
    "\n",
    "# # Ajouter une colonne pour extraire l'heure de l'événement\n",
    "# df['event_hour'] = df['event_time'].dt.hour\n",
    "\n",
    "# # Appliquer la fonction sur toute la colonne\n",
    "# df['most_active_time'] = df['event_hour'].apply(time_of_day, meta=('event_hour', 'object'))\n",
    "\n",
    "# # Calculer les statistiques pour une période donnée\n",
    "# def compute_user_stats(df, period_label, period_offset):\n",
    "#     period_df = df[df['event_time'] >= df['event_time'].max() - pd.DateOffset(months=period_offset)]\n",
    "    \n",
    "#     # Calculer les vues, les paniers et les achats\n",
    "#     number_of_views = period_df[period_df['event_type'] == 'view'].groupby('user_id').size().rename(f'number_of_views_{period_label}')\n",
    "#     number_of_carts = period_df[period_df['event_type'] == 'cart'].groupby('user_id').size().rename(f'number_of_carts_{period_label}')\n",
    "#     count_products = period_df[period_df['event_type'] == 'purchase'].groupby('user_id').size().rename(f'count_products_{period_label}')\n",
    "    \n",
    "#     # Calculer le prix moyen des achats\n",
    "#     avg_price = period_df[period_df['event_type'] == 'purchase'].groupby('user_id')['price'].mean().rename(f'avg_price_{period_label}')\n",
    "    \n",
    "#     # Calculer le nombre de sessions\n",
    "#     number_of_sessions = period_df.groupby('user_id')['user_session'].nunique().rename(f'number_of_sessions_{period_label}')\n",
    "    \n",
    "#     # Fusionner toutes les statistiques\n",
    "#     stats = dd.concat([number_of_views, number_of_carts, count_products, avg_price, number_of_sessions], axis=1)\n",
    "#     return stats\n",
    "\n",
    "# # Calculer les statistiques pour les périodes de 2, 5, et 7 mois\n",
    "# stats_2m = compute_user_stats(df, '2m', 2)\n",
    "# stats_5m = compute_user_stats(df, '5m', 5)\n",
    "# stats_7m = compute_user_stats(df, '7m', 7)\n",
    "\n",
    "# # Fusionner toutes les statistiques ensemble\n",
    "# user_stats_df = stats_2m.merge(stats_5m, on='user_id', how='outer').merge(stats_7m, on='user_id', how='outer')\n",
    "\n",
    "# # Calculer les statistiques supplémentaires\n",
    "# last_purchase = df[df['event_type'] == 'purchase'].groupby('user_id')['event_time'].max()\n",
    "# days_since_last_purchase = (df['event_time'].max() - last_purchase).dt.days\n",
    "# total_purchase_value = df[df['event_type'] == 'purchase'].groupby('user_id')['price'].sum()\n",
    "\n",
    "# # Ajouter les statistiques supplémentaires au DataFrame\n",
    "# user_stats_df['last_purchase_temp'] = last_purchase\n",
    "# user_stats_df['days_since_last_purchase'] = days_since_last_purchase\n",
    "# user_stats_df['total_purchase_value'] = total_purchase_value\n",
    "\n",
    "# # Calculer les abandons de panier\n",
    "# cart_count = df[df['event_type'] == 'cart'].groupby('user_id').size()\n",
    "# purchase_count = df[df['event_type'] == 'purchase'].groupby('user_id').size()\n",
    "# cart_abandonments = cart_count - purchase_count\n",
    "\n",
    "# # Utiliser .where pour remplacer les valeurs négatives par 0\n",
    "# cart_abandonments = cart_abandonments.where(cart_abandonments >= 0, 0)\n",
    "\n",
    "# user_stats_df['cart_count'] = cart_count\n",
    "# user_stats_df['purchase_count'] = purchase_count\n",
    "# user_stats_df['cart_abandonments'] = cart_abandonments\n",
    "\n",
    "# # Convertir la partie nécessaire en Pandas pour calculer les préférences de marque et de catégorie\n",
    "# df_pandas = df[df['event_type'] == 'purchase'].compute()\n",
    "\n",
    "# # Calculer les préférences de marque\n",
    "# preferred_brand = df_pandas.groupby(['user_id', 'brand']).size().reset_index()\n",
    "# preferred_brand.columns = ['user_id', 'brand', 'count']  # Renommer les colonnes correctement\n",
    "# preferred_brand = preferred_brand.loc[preferred_brand.groupby('user_id')['count'].idxmax()].set_index('user_id')['brand']\n",
    "\n",
    "# # Calculer les préférences de catégorie\n",
    "# preferred_category = df_pandas.groupby(['user_id', 'category_code']).size().reset_index()\n",
    "# preferred_category.columns = ['user_id', 'category_code', 'count']  # Renommer les colonnes correctement\n",
    "# preferred_category = preferred_category.loc[preferred_category.groupby('user_id')['count'].idxmax()].set_index('user_id')['category_code']\n",
    "\n",
    "# # Ajouter les préférences au DataFrame principal\n",
    "# user_stats_df['preferred_brand'] = preferred_brand\n",
    "# user_stats_df['preferred_category'] = preferred_category\n",
    "\n",
    "# # Calculer la période la plus active pour chaque utilisateur\n",
    "# most_active_time_df = df.groupby(['user_id', 'most_active_time']).size().reset_index()\n",
    "# most_active_time_df.columns = ['user_id', 'most_active_time', 'activity_count']  # Renommer la colonne de taille pour éviter les erreurs\n",
    "\n",
    "# # Calculer le rang de l'activité\n",
    "# most_active_time_df['rank'] = most_active_time_df.groupby('user_id')['activity_count'].rank(method='first', ascending=False)\n",
    "\n",
    "# # Filtrer pour ne garder que la période la plus active\n",
    "# most_active_time_df = most_active_time_df[most_active_time_df['rank'] == 1].drop(columns=['rank', 'activity_count'])\n",
    "\n",
    "# # Supprimer la colonne 'most_active_time' s'il existe déjà dans user_stats_df\n",
    "# if 'most_active_time' in user_stats_df.columns:\n",
    "#     user_stats_df = user_stats_df.drop(columns=['most_active_time'])\n",
    "\n",
    "# # Réaliser la jointure\n",
    "# user_stats_df = user_stats_df.merge(most_active_time_df[['user_id', 'most_active_time']], on='user_id', how='inner')\n",
    "\n",
    "# # Vérification des colonnes\n",
    "# print(user_stats_df.columns)\n",
    "\n",
    "# # Afficher les premières lignes\n",
    "# print(user_stats_df.head().compute())  # Utilisez compute() avant d'afficher les résultats\n",
    "\n",
    "# # Optionnel : Sauvegarder le DataFrame final\n",
    "# user_stats_df.to_parquet('./data/user_stats_df_final.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
