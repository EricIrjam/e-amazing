{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73c4887-ad90-47c7-b7a1-74f971b7f06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/20 05:28:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/jovyan/work/filtered_df_output.parquet\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"E-commerce Amazing Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Augmenter la fraction de mémoire allouée aux tâches d'exécution\n",
    "spark.conf.set(\"spark.memory.fraction\", \"0.8\")\n",
    "\n",
    "# Ajuster la fraction de mémoire réservée pour la gestion interne\n",
    "spark.conf.set(\"spark.memory.storageFraction\", \"0.2\")\n",
    "\n",
    "from pyspark.sql.functions import row_number, col, split, month, year, datediff, sum as _sum,\\\n",
    "    avg as _avg, count as _count, max as _max, current_date, round as _round, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Exemple de repartitionnement des données\n",
    "filtered_df = filtered_df.repartition(100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ajouter les colonnes 'month' et 'year'\n",
    "filtered_df = filtered_df.withColumn(\"month\", month(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"year\", year(col(\"event_time\")))\n",
    "\n",
    "# Filtrer les événements d'achat\n",
    "purchase_df = filtered_df.filter(col(\"event_type\") == \"purchase\")\n",
    "\n",
    "# Extraire les paires uniques category_id et category_code\n",
    "category_mapping_df = purchase_df.select(\"category_id\", \"category_code\").distinct()\n",
    "\n",
    "# Sauvegarder ce mapping dans un fichier Parquet pour une utilisation ultérieure\n",
    "mapping_output_path = \"/home/jovyan/work/category_mapping.parquet\"\n",
    "category_mapping_df.write.mode(\"overwrite\").parquet(mapping_output_path)\n",
    "\n",
    "# Charger le mapping depuis le fichier Parquet\n",
    "category_mapping_df = spark.read.parquet(mapping_output_path)\n",
    "\n",
    "# Renommer la colonne 'category_code' dans le DataFrame de mapping pour éviter l'ambiguïté\n",
    "category_mapping_df = category_mapping_df.withColumnRenamed(\"category_code\", \"mapped_category_code\")\n",
    "\n",
    "# Joindre purchase_df avec category_mapping_df pour ajouter la colonne 'mapped_category_code'\n",
    "purchase_df_with_mapping = purchase_df.join(category_mapping_df, on=\"category_id\", how=\"left\")\n",
    "\n",
    "# Remplacer les valeurs NULL dans 'category_code' par les valeurs correspondantes de la jointure\n",
    "purchase_df = purchase_df_with_mapping.withColumn(\n",
    "    \"category_code\",\n",
    "    coalesce(purchase_df_with_mapping[\"category_code\"], purchase_df_with_mapping[\"mapped_category_code\"])\n",
    ")\n",
    "\n",
    "# Calcul des variables explicatives supplémentaires\n",
    "# 1. Nombre de vues par utilisateur\n",
    "number_of_views = filtered_df.filter(col(\"event_type\") == \"view\") \\\n",
    "                             .groupBy(\"user_id\") \\\n",
    "                             .agg(_count(\"event_type\").alias(\"number_of_views\"))\n",
    "\n",
    "# 2. Nombre de produits ajoutés au panier par utilisateur\n",
    "number_of_carts = filtered_df.filter(col(\"event_type\") == \"cart\") \\\n",
    "                             .groupBy(\"user_id\") \\\n",
    "                             .agg(_count(\"event_type\").alias(\"number_of_carts\"))\n",
    "\n",
    "# 3. Nombre total d'achats précédents par utilisateur\n",
    "previous_purchases = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                .groupBy(\"user_id\") \\\n",
    "                                .agg(_count(\"event_type\").alias(\"user_previous_purchases\"))\n",
    "\n",
    "# 4. Valeur moyenne des achats précédents par utilisateur\n",
    "average_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                    .groupBy(\"user_id\") \\\n",
    "                                    .agg(_round(_avg(\"price\"), 2).alias(\"user_average_purchase_value\"))\n",
    "\n",
    "# 5. Temps écoulé depuis le dernier achat\n",
    "last_purchase_date = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                .groupBy(\"user_id\") \\\n",
    "                                .agg(_max(\"event_time\").alias(\"last_purchase_date\"))\n",
    "\n",
    "days_since_last_purchase = last_purchase_date.withColumn(\"days_since_last_purchase\", \n",
    "                                                         datediff(current_date(), col(\"last_purchase_date\")))\n",
    "\n",
    "# 6. Nombre de produits ajoutés au panier mais non achetés (abandons de panier)\n",
    "cart_abandonments = filtered_df.filter(col(\"event_type\") == \"cart\") \\\n",
    "                               .groupBy(\"user_id\", \"product_id\") \\\n",
    "                               .agg(_count(\"event_type\").alias(\"cart_count\")) \\\n",
    "                               .join(purchase_df.groupBy(\"user_id\", \"product_id\").agg(_count(\"event_type\").alias(\"purchase_count\")),\n",
    "                                     on=[\"user_id\", \"product_id\"], how=\"left\") \\\n",
    "                               .withColumn(\"purchase_count\", col(\"purchase_count\").cast(\"int\")) \\\n",
    "                               .na.fill(0) \\\n",
    "                               .filter(col(\"purchase_count\") == 0) \\\n",
    "                               .groupBy(\"user_id\") \\\n",
    "                               .agg(_count(\"product_id\").alias(\"cart_abandonments\"))\n",
    "\n",
    "# 7. Valeur totale des achats par utilisateur\n",
    "total_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                  .groupBy(\"user_id\") \\\n",
    "                                  .agg(_round(_sum(\"price\"), 2).alias(\"total_purchase_value\"))\n",
    "\n",
    "# 8. Nombre total de sessions par utilisateur\n",
    "number_of_sessions = filtered_df.groupBy(\"user_id\") \\\n",
    "                                .agg(_count(\"user_session\").alias(\"number_of_sessions\"))\n",
    "\n",
    "# Joindre les DataFrames pour créer le DataFrame final\n",
    "final_df = number_of_views.join(number_of_carts, \"user_id\", \"left\") \\\n",
    "                          .join(previous_purchases, \"user_id\", \"left\") \\\n",
    "                          .join(average_purchase_value, \"user_id\", \"left\") \\\n",
    "                          .join(days_since_last_purchase, \"user_id\", \"left\") \\\n",
    "                          .join(cart_abandonments, \"user_id\", \"left\") \\\n",
    "                          .join(total_purchase_value, \"user_id\", \"left\") \\\n",
    "                          .join(number_of_sessions, \"user_id\", \"left\")\n",
    "\n",
    "# Remplacer les valeurs NULL par des valeurs par défaut si nécessaire\n",
    "final_df = final_df.fillna({\n",
    "    \"number_of_views\": 0,\n",
    "    \"number_of_carts\": 0,\n",
    "    \"user_previous_purchases\": 0,\n",
    "    \"user_average_purchase_value\": 0.0,\n",
    "    \"days_since_last_purchase\": 9999,  # Utiliser une valeur par défaut pour indiquer une absence de précédent achat\n",
    "    \"cart_abandonments\": 0,\n",
    "    \"total_purchase_value\": 0.0,\n",
    "    \"number_of_sessions\": 0\n",
    "})\n",
    "\n",
    "# Sauvegarder le DataFrame final dans un fichier Parquet\n",
    "final_output_path = \"/home/jovyan/work/final_df_output.parquet\"\n",
    "final_df.write.mode(\"overwrite\").parquet(final_output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21dd156-ee34-47c7-b6d6-4879bf81a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+-----------------------+---------------------------+-------------------+------------------------+-----------------+--------------------+------------------+\n",
      "|  user_id|number_of_views|number_of_carts|user_previous_purchases|user_average_purchase_value| last_purchase_date|days_since_last_purchase|cart_abandonments|total_purchase_value|number_of_sessions|\n",
      "+---------+---------------+---------------+-----------------------+---------------------------+-------------------+------------------------+-----------------+--------------------+------------------+\n",
      "|512609101|             39|              1|                      0|                        0.0|               NULL|                    9999|                1|                 0.0|                40|\n",
      "|515230351|            123|              1|                      0|                        0.0|               NULL|                    9999|                1|                 0.0|               124|\n",
      "|519643001|            747|             38|                      6|                      61.59|2020-03-12 06:54:52|                    1561|               15|              369.54|               791|\n",
      "|532218441|           2354|             21|                      7|                     204.73|2020-03-13 12:53:08|                    1560|                8|             1433.14|              2382|\n",
      "|540921161|             13|              0|                      0|                        0.0|               NULL|                    9999|                0|                 0.0|                13|\n",
      "|542818291|             43|              9|                      2|                      299.5|2020-02-27 09:23:24|                    1575|                2|               599.0|                54|\n",
      "|546906941|             41|              0|                      0|                        0.0|               NULL|                    9999|                0|                 0.0|                41|\n",
      "|547783521|            172|              2|                      0|                        0.0|               NULL|                    9999|                2|                 0.0|               174|\n",
      "|549859831|            171|             25|                     18|                     240.48|2020-03-01 11:17:09|                    1572|                1|             4328.57|               214|\n",
      "|572480341|            246|             63|                     21|                     170.36|2020-03-17 03:15:05|                    1556|                6|             3577.47|               330|\n",
      "|584878141|           1737|              5|                      1|                     480.11|2019-12-20 15:19:44|                    1644|                4|              480.11|              1743|\n",
      "|586693081|            707|              8|                      0|                        0.0|               NULL|                    9999|                7|                 0.0|               715|\n",
      "|589672011|            102|              2|                      0|                        0.0|               NULL|                    9999|                2|                 0.0|               104|\n",
      "|602370711|            333|              5|                      1|                     577.69|2020-03-02 14:32:38|                    1571|                2|              577.69|               339|\n",
      "|603075391|            369|              0|                      0|                        0.0|               NULL|                    9999|                0|                 0.0|               369|\n",
      "|607601041|            170|              4|                      0|                        0.0|               NULL|                    9999|                4|                 0.0|               174|\n",
      "|608530751|            228|              2|                      0|                        0.0|               NULL|                    9999|                2|                 0.0|               230|\n",
      "|608705471|            134|              0|                      0|                        0.0|               NULL|                    9999|                0|                 0.0|               134|\n",
      "|610774301|             27|              0|                      0|                        0.0|               NULL|                    9999|                0|                 0.0|                27|\n",
      "|611696531|           1058|             68|                      7|                      44.49|2020-04-23 05:25:09|                    1519|                9|              311.44|              1133|\n",
      "+---------+---------------+---------------+-----------------------+---------------------------+-------------------+------------------------+-----------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b410c2-843b-47a1-a88e-c19aaad3ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les noms des colonnes du DataFrame sont :\n",
      "event_time\n",
      "event_type\n",
      "product_id\n",
      "category_id\n",
      "category_code\n",
      "brand\n",
      "price\n",
      "user_id\n",
      "user_session\n"
     ]
    }
   ],
   "source": [
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Afficher les noms des colonnes\n",
    "columns = filtered_df.columns\n",
    "print(\"Les noms des colonnes du DataFrame sont :\")\n",
    "for column in columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b49e0-7779-439c-861a-476625915e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76191b63-8a05-48ae-a2df-127597d2b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les noms des colonnes du DataFrame modifié sont :\n",
      "user_id\n",
      "event_time\n",
      "event_type\n",
      "product_id\n",
      "category_id\n",
      "category_code\n",
      "brand\n",
      "price\n",
      "user_session\n",
      "event_year\n",
      "event_month\n",
      "event_day_of_week\n",
      "event_hour\n",
      "event_weekend\n",
      "price_category\n",
      "time_of_day\n",
      "device_type\n",
      "user_segment\n",
      "preferred_category\n",
      "preferred_brand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 05:51:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:51:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:51:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:51:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:52:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:53:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:54:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:55:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:55:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:55:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:55:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:55:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:55:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 05:56:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 76:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+----------+-------------------+--------------------+--------+-------+--------------------+----------+-----------+-----------------+----------+-------------+--------------+-----------+-----------+------------+------------------+---------------+\n",
      "|  user_id|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|        user_session|event_year|event_month|event_day_of_week|event_hour|event_weekend|price_category|time_of_day|device_type|user_segment|preferred_category|preferred_brand|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+--------+-------+--------------------+----------+-----------+-----------------+----------+-------------+--------------+-----------+-----------+------------+------------------+---------------+\n",
      "|220134341|2020-01-21 14:26:08|      view|  12300304|2053013563743667055|appliances.kitche...|  alteco|  43.24|d393e798-57d7-4f1...|      2020|          1|                3|        14|      weekday|           low|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-01 12:24:46|      view|  21404526|2232732082063278200|  electronics.clocks|   casio|  26.47|05c839ae-463c-444...|      2020|          1|                4|        12|      weekday|           low|    morning|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-21 14:43:22|      view| 100031495|2053013553526341921|appliances.ironin...|    skad|  232.7|d74c9495-b8b6-4dc...|      2020|          1|                3|        14|      weekday|          high|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-21 14:49:01|      view|  15800217|2232732108613223108|       sport.trainer|  berkut| 200.78|d74c9495-b8b6-4dc...|      2020|          1|                3|        14|      weekday|          high|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-21 14:49:50|      view|  15800219|2232732108613223108|       sport.trainer|  berkut|  149.3|d74c9495-b8b6-4dc...|      2020|          1|                3|        14|      weekday|        medium|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-21 14:50:35|      view|  15800220|2232732108613223108|       sport.trainer|  berkut| 164.74|d74c9495-b8b6-4dc...|      2020|          1|                3|        14|      weekday|        medium|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-21 14:51:49|      view|  15800180|2232732108613223108|       sport.trainer|  p.i.t.|  127.7|d74c9495-b8b6-4dc...|      2020|          1|                3|        14|      weekday|        medium|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-01 12:43:16|      view|  21402651|2232732082063278200|  electronics.clocks|  fossil| 144.15|867e5b57-dfb9-43d...|      2020|          1|                4|        12|      weekday|        medium|    morning|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-21 14:52:28|      view|  15800217|2232732108613223108|       sport.trainer|  berkut| 200.78|d74c9495-b8b6-4dc...|      2020|          1|                3|        14|      weekday|          high|  afternoon|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "|220134341|2020-01-01 12:46:17|      view|  21410498|2053013561579406073|  electronics.clocks|longines|1328.22|b50aeb53-d3dd-4d7...|      2020|          1|                4|        12|      weekday|          high|    morning|     mobile|high spender|accessories.wallet|         sho-me|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+--------+-------+--------------------+----------+-----------+-----------------+----------+-------------+--------------+-----------+-----------+------------+------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, year, month, dayofweek, hour, when, count, max as _max, lit\n",
    "\n",
    "\n",
    "\n",
    "# Lire le fichier Parquet\n",
    "filtered_df = spark.read.parquet(output_path)\n",
    "\n",
    "# Extraire les colonnes nécessaires\n",
    "filtered_df = filtered_df.select(\"event_time\", \"event_type\", \"product_id\", \"category_id\", \"category_code\", \"brand\", \"price\", \"user_id\", \"user_session\")\n",
    "\n",
    "# Créer de nouvelles variables qualitatives\n",
    "filtered_df = filtered_df.withColumn(\"event_year\", year(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_month\", month(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_day_of_week\", dayofweek(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_hour\", hour(col(\"event_time\"))) \\\n",
    "                         .withColumn(\"event_weekend\", when(col(\"event_day_of_week\").isin([1, 7]), \"weekend\").otherwise(\"weekday\")) \\\n",
    "                         .withColumn(\"price_category\", when(col(\"price\") < 50, \"low\") \\\n",
    "                                                        .when((col(\"price\") >= 50) & (col(\"price\") < 200), \"medium\") \\\n",
    "                                                        .otherwise(\"high\")) \\\n",
    "                         .withColumn(\"time_of_day\", when(col(\"event_hour\").between(0, 6), \"night\") \\\n",
    "                                                    .when(col(\"event_hour\").between(7, 12), \"morning\") \\\n",
    "                                                    .when(col(\"event_hour\").between(13, 18), \"afternoon\") \\\n",
    "                                                    .otherwise(\"evening\"))\n",
    "\n",
    "# Calculer la valeur moyenne des achats pour déterminer le segment utilisateur\n",
    "average_purchase_value = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                    .groupBy(\"user_id\") \\\n",
    "                                    .agg(_avg(\"price\").alias(\"avg_purchase_value\"))\n",
    "\n",
    "average_purchase_value = average_purchase_value.withColumn(\"user_segment\", when(col(\"avg_purchase_value\") >= 100, \"high spender\").otherwise(\"regular buyer\"))\n",
    "\n",
    "# Identifier la catégorie de produit préférée de l'utilisateur\n",
    "preferred_category = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                                .groupBy(\"user_id\", \"category_code\") \\\n",
    "                                .agg(count(\"category_code\").alias(\"category_count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"category_count\").desc())\n",
    "preferred_category = preferred_category.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                                       .filter(col(\"rank\") == 1) \\\n",
    "                                       .select(\"user_id\", \"category_code\") \\\n",
    "                                       .withColumnRenamed(\"category_code\", \"preferred_category\")\n",
    "\n",
    "# Indicateur de fidélité à une marque spécifique\n",
    "brand_loyalty = filtered_df.filter(col(\"event_type\") == \"purchase\") \\\n",
    "                           .groupBy(\"user_id\", \"brand\") \\\n",
    "                           .agg(count(\"brand\").alias(\"brand_count\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"brand_count\").desc())\n",
    "brand_loyalty = brand_loyalty.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                             .filter(col(\"rank\") == 1) \\\n",
    "                             .select(\"user_id\", \"brand\") \\\n",
    "                             .withColumnRenamed(\"brand\", \"preferred_brand\")\n",
    "\n",
    "# Ajouter une colonne factice pour le type d'appareil (exemple)\n",
    "# Note: Cela nécessite d'avoir une colonne dans les données initiales qui identifie le type d'appareil.\n",
    "# Pour l'exemple, nous allons générer une colonne aléatoire.\n",
    "import random\n",
    "device_types = [\"mobile\", \"desktop\", \"tablet\"]\n",
    "filtered_df = filtered_df.withColumn(\"device_type\", lit(random.choice(device_types)))\n",
    "\n",
    "# Joindre les nouvelles variables qualitatives au DataFrame final\n",
    "final_df = filtered_df.join(average_purchase_value.select(\"user_id\", \"user_segment\"), on=\"user_id\", how=\"left\") \\\n",
    "                      .join(preferred_category, on=\"user_id\", how=\"left\") \\\n",
    "                      .join(brand_loyalty, on=\"user_id\", how=\"left\") \\\n",
    "                      .distinct()  # Suppression des duplicatas si nécessaire\n",
    "\n",
    "# Afficher les noms des colonnes pour vérifier les nouvelles variables\n",
    "print(\"Les noms des colonnes du DataFrame modifié sont :\")\n",
    "for column in final_df.columns:\n",
    "    print(column)\n",
    "\n",
    "# Afficher un échantillon des données pour vérifier les nouvelles variables\n",
    "final_df.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345a808-ad4c-4531-ab37-c013d14aa6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269b4d4-bae9-4f7e-bb19-c8728b617471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d03c4-9e1a-4045-8231-5012bb045d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484f396-80e3-400a-a752-46223641ae83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87d0f4-bdd6-447f-bc8a-b147bd65f915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cfeab-6dc7-48af-bcec-d61b7e2929d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêter la session Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
